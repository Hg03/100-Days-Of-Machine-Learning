{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Hi, my name is Harish Gehlot and I am Data Science and Machine Learning Enthusiast. We are seeing that 100 days of machine learning is getting popular. But this is a serious challenge of discipline \ud83d\udcdc and getting too much knowledge \ud83d\udc68\u200d\ud83d\udd2c. So here I've designed a 100 days structure in which what I am doing everyday is discussed. So if you are also interested \ud83c\udf83, Let's start this journey together.</p>"},{"location":"#structure","title":"Structure \ud83e\ude96","text":"<ul> <li> <p>Day 1</p> <ul> <li><code>In Day 1, we'll learn about Basic concepts of Machine Learning.</code></li> <li><code>What is Machine learning ?.</code></li> <li><code>Supervised, Unsupervised and Reinforcement Learning .</code></li> <li><code>Basic Terminologies used in Machine Learning .</code></li> </ul> </li> <li> <p>Day 2</p> <ul> <li><code>Some basic Machine Learning Interview questions .</code></li> <li><code>Difference between Supervised and Unsupervised Machine learning algorithm ?</code></li> <li><code>Difference between Data Engineer, Data Scienctist, Data Analyst and Machine Learning Engineer ?</code></li> <li><code>What is Online and Offline learning ?</code></li> <li><code>How Machine Learning is different from Deep Learning ?</code></li> </ul> </li> <li> <p>Day 3</p> <ul> <li><code>Machine Learning Lifecycle and their different aspects .</code></li> </ul> </li> <li> <p>Day 4</p> <ul> <li><code>Analyze the Data and extract some insights i.e. Exploratory Data Analysis .</code></li> <li><code>Use some dataset to visualize and extract information using python library .</code></li> </ul> </li> <li> <p>Day 5</p> <ul> <li><code>Learn different methods to impute missing values (Numerical and categorical features).</code></li> </ul> </li> <li> <p>Day 6</p> <ul> <li><code>Learn different methods to encode the categorical features .</code></li> </ul> </li> <li> <p>Day 7</p> <ul> <li><code>Learn different methods to remove outliers.</code></li> </ul> </li> <li> <p>Day 8</p> <ul> <li><code>Learn different methods for feature selection.</code></li> </ul> </li> <li> <p>Day 9</p> <ul> <li><code>Generate a sequential pipeline to transform the dataset .</code></li> </ul> </li> <li> <p>Day 10 to 15</p> <ul> <li><code>Learn and Implement Linear Regression Algorithm .</code></li> <li><code>Machine learning interview question regarding this algorithm .</code></li> </ul> </li> <li> <p>Day 16 to 20</p> <ul> <li><code>Logistic Regression</code></li> <li><code>Machine learning interview question regarding this algorithm .</code></li> </ul> </li> <li> <p>Day 21 to 25</p> <ul> <li><code>Support Vector Machine</code></li> </ul> </li> <li> <p>Day 26 to 34</p> <ul> <li><code>Decision Tree Algorithm</code></li> <li><code>Random Forest Algorithm</code></li> <li><code>Machine learning interview question regarding this algorithms .</code></li> </ul> </li> <li> <p>Day 35 to 40</p> <ul> <li><code>ADA Boost and Gradient Boost Algorithm .</code></li> <li><code>Machine learning interview question regarding this algorithm .</code></li> </ul> </li> <li> <p>Day 41 to 44</p> <ul> <li><code>Naive Bayes Algorithm .</code></li> <li><code>Machine learning interview question regarding this algorithm .</code></li> </ul> </li> <li> <p>Day 45 to 46</p> <ul> <li><code>KNN Algorithm i.e K Nearest Neighbor</code></li> </ul> </li> <li> <p>Day 47 to 49</p> <ul> <li><code>Learn about different metrics that we can use to check the accuracy of the model.</code></li> </ul> </li> <li> <p>Day 50 to 54 (Building First Machine Learning Regression Project)</p> <ul> <li><code>Proceeding the whole ML lifecycle (without MLOps) and build the Regression model.</code></li> <li><code>Preprocess the data and identify which ML algorith to choose according to the metrics that we've used .</code></li> </ul> </li> <li> <p>Day 55 to 57 (Building Second Machine Learning Classification Project)</p> <ul> <li><code>Proceeding the whole ML lifecycle (without MLOps) and build the Classification model.</code></li> <li><code>Preprocess the data and identify which ML algorith to choose according to the metrics that we've used .</code></li> </ul> </li> <li> <p>Day 58 to 59 (Building Visualization Dashboard)</p> <ul> <li><code>Visualize the dataset and create an insightful dashboard .</code></li> </ul> </li> <li> <p>Day 60</p> <ul> <li><code>Let's move to unsupervised learning ?</code></li> <li><code>Different Unsupervised Machine Learning Algorithms ?</code></li> <li><code>K Means Clustering Algorithm</code></li> </ul> </li> <li> <p>Day 61</p> <ul> <li><code>Implementing K Means Clustering algorithm</code></li> </ul> </li> <li> <p>Day 62 (Building Recommendation System)</p> <ul> <li><code>Implement the Movie genre recommendation system .</code></li> </ul> </li> <li> <p>Day 63</p> <ul> <li><code>What is Deep Learning ?</code></li> <li><code>What is Neural Network ?</code></li> <li><code>Implementing First Neural Network .</code></li> </ul> </li> <li> <p>Day 64</p> <ul> <li><code>Break down the Neural network that we've built in python .</code></li> </ul> </li> </ul>"},{"location":"Day%201/","title":"Day 1","text":""},{"location":"Day%201/#concepts-and-terminologies","title":"Concepts and Terminologies","text":"<p>What is Machine Learning and why Is It everywhere ?</p> <p>As the word says Machine and it's Learning means we have a machine or we can say function which takes some input values and predicts or calculates the answer. Easy \ud83c\udf6c\ud83c\udf6c ??</p> <p>Let's take an example of our brain \ud83d\udd3d</p> <p>Suppose you are student studying in senior secondary school, and you have a exam in somedays and you need to score good marks \ud83d\udcaf . For these, what you'll do, grab some books and study. By studying, internally your brain is learning pattern that which type of question can occurs, how to solve them. Here comes one term that is training. You are training \ud83d\ude86\ud83d\ude86 your brain \ud83e\udde0 to predict or calculate the answers for your upcoming exams. More your brain learns, more it will become accurate to the results. So we discussed is the layman's term for define Machine learning .</p> <p>To be more technical, A machine \ud83d\udce0 is a system or module which takes data \ud83d\uddc3\ufe0f in form of csv, excel file, some documents like docx or pdf, some images or anything. Then we'll clear / filter or preprocess \ud83d\udc87 the data, Then our machine learns crucial patterns \ud83d\udcc8 regarding that data like it builds some equations assigning some weights to some parameters. After learning or we can say Model training and after training, it predicts the results for unknown data by applying same pattern that it has learnt before.</p> <p>Now let's get some familiarity with Machine Learning terminologies - </p> <ul> <li> <p><code>Data \ud83d\udcca</code> : The raw information or observations used for training and testing machine learning models. Data can be in various forms, such as text, images, numbers, or more.</p> </li> <li> <p><code>Feature \ud83d\udd0d</code> : An individual input variable or attribute used in a machine learning model to make predictions or classifications. Features are derived from the data.</p> </li> <li> <p><code>Label or Target \ud83c\udfaf</code> : The output or outcome that a machine learning model predicts or classifies. In supervised learning, the model is trained to predict the label.</p> </li> <li> <p><code>Training Data \ud83d\udcda</code> : A subset of the data used to train a machine learning model. It includes both the features and their corresponding labels.</p> </li> <li> <p><code>Testing Data \ud83e\uddea</code> : A separate subset of the data used to evaluate the performance of a machine learning model after it has been trained.</p> </li> <li> <p><code>Algorithm \ud83e\uddee</code> : A set of rules and procedures that a machine learning model follows to learn from data and make predictions or decisions. Examples include decision trees, neural networks, and k-nearest neighbors.</p> </li> <li> <p><code>Model \ud83e\udd2f</code> : The result of training a machine learning algorithm on data. It represents the learned patterns and relationships in the data.</p> </li> <li> <p><code>Supervised Learning \ud83d\udc65</code> : A type of machine learning where the model is trained on labeled data, and its goal is to learn a mapping from input features to output labels.</p> </li> <li> <p><code>Unsupervised Learning \ud83e\udd37\u200d\u2642\ufe0f</code> : A type of machine learning where the model is trained on unlabeled data, and its goal is to discover patterns or structure in the data without specific output labels.</p> </li> <li> <p><code>Classification \ud83c\udff7\ufe0f</code> : A type of supervised learning task where the goal is to assign data points to predefined categories or classes.</p> </li> <li> <p><code>Regression \ud83d\udcc8</code> : A type of supervised learning task where the goal is to predict a continuous numeric value as the output.</p> </li> <li> <p><code>Overfitting \ud83d\ude45\u200d\u2642\ufe0f</code> : Occurs when a machine learning model performs well on the training data but poorly on new, unseen data because it has learned noise or irrelevant patterns.</p> </li> <li> <p><code>Underfitting \ud83d\ude45\u200d\u2642\ufe0f</code> : Occurs when a machine learning model is too simple to capture the underlying patterns in the data and performs poorly on both training and testing data.</p> </li> <li> <p><code>Accuracy \u2705</code> : A common evaluation metric that measures the proportion of correctly predicted instances in a classification task.</p> </li> <li> <p><code>Loss Function \ud83d\udcc9</code> : A mathematical function that quantifies the error between the model's predictions and the actual target values, used during training to optimize the model.</p> </li> <li> <p><code>Feature Engineering \ud83d\udd27</code> : The process of selecting, transforming, or creating new features from the raw data to improve a machine learning model's performance.</p> </li> <li> <p><code>Validation Set \ud83d\udd04</code> : A subset of the data used during model training to tune hyperparameters and avoid overfitting.</p> </li> </ul>"},{"location":"Day%201/#dive-into-supervised-learning","title":"Dive into Supervised Learning","text":"<p>Supervised Machine Learning \ud83c\udfaf is a type of Machine Learning in which algorithmic model trains on a dataset which have independent features and target both. So in this category of machine learning,  model will analyze the patterns and understand how target value can be generated / calculated using independent features.</p> <p><code>For example :-</code></p> <ul> <li> <p>Diabetes prediction \ud83e\uddcb - Suppose we have a dataset having independent features like <code>cholestrol</code>, <code>Blood pressure</code>, <code>systolic pressure</code> etc. and one target feature depicting that person is diabetic or not.</p> </li> <li> <p>House Price Prediction \ud83c\udfda\ufe0f - Suppose we have a dataset having independent features like <code>number of rooms</code>, <code>area (in square km)</code> etc. and one target feature depicting the price of house.</p> </li> </ul>"},{"location":"Day%201/#dive-into-unsupervised-learning","title":"Dive into Unsupervised Learning","text":"<p>Unsupervised Machine Learning \ud83d\udd34 is a type of Machine Learning in which algorithmic model trains on a unlabelled dataset (means no target) and model will try to identify pattern and create different clusters based on different pattern model observe.</p> <p><code>For example :-</code></p> <ul> <li><code>revenue prediction</code> - An example of unsupervised machine learning would be a case where a supermarket wants to increase its revenue. It decides to implement a machine learning algorithm on its sold products\u2019 data. It was observed that the customers who bought cereals more often tend to buy milk or those who buy eggs tend to buy bacon. Thus, redesigning the store and placing related products side by side can help them understand consumer mindset and increase revenue.</li> </ul>"},{"location":"Day%201/#dive-into-reinforcement-learning","title":"Dive into Reinforcement Learning","text":"<p>Reinforcement Machine Learning \ud83c\udf2a\ufe0f is a type of machine learning where an agent learns to make sequential decisions to maximize a cumulative reward. It is commonly used in scenarios where an agent interacts with an environment and learns to take actions that lead to the most desirable outcomes over time.</p> <p><code>For example :-</code></p> <p>Game playing (e.g., AlphaGo, which learned to play the board game Go), autonomous driving (teaching a self-driving car to navigate safely), robotics (training a robot to perform tasks), and recommendation systems (learning to recommend products or content to users).</p>"},{"location":"Day%201/#resource","title":"Resource \ud83e\udeb5","text":"<p>You can learn more about types of machine learning through this video \ud83d\udcfc</p>"},{"location":"Day%2010/","title":"Day 10","text":""},{"location":"Day%2010/#what-is-linear-regression","title":"What is Linear Regression \ud83d\udca1 \ud83d\udca1","text":"<p>Referring</p> <p>Linear Regression is a simple and powerful model for predicting a numeric response from a set of one or more independent variables. This article will focus mostly on how the method is used in machine learning, so we won't cover common use cases like causal inference or experimental design. And although it may seem like linear regression is overlooked in modern machine learning's ever-increasing world of complex neural network architectures, the algorithm is still widely used across a large number of domains because it is effective, easy to interpret, and easy to extend. The key ideas in linear regression are recycled everywhere, so understanding the algorithm is a must-have for a strong foundation in machine learning.</p>"},{"location":"Day%2010/#lets-be-more-specific","title":"Let's Be More Specific","text":"<p>Linear regression is a supervised algorithm that learns to model a dependent variable, y, as a function of some independent variables (aka \"features\"), x<sub>i</sub>\u200b, by finding a line (or surface) that best \"fits\" the data. In general, we assume y to be some number and each x<sub>i</sub> can be basically anything. For example: predicting the price of a house using the number of rooms in that house (y : price, x<sub>1</sub> \u200b: number of rooms) or predicting weight from height and age (y : weight, x<sub>1</sub> : height, x<sub>2</sub> : age).</p> <p>In general, the equation for linear regression is</p> <p>y = \u03b2<sub>0</sub> \u200b+ \u03b2<sub>1</sub> \u200bx<sub>1</sub> \u200b + \u03b2<sub>2</sub>\u200bx<sub>2</sub> \u200b+ ... + \u03b2<sub>p</sub>\u200bx<sub>p</sub>\u200b + \u03f5</p> <p>where:</p> <ul> <li>y: the dependent variable; the thing we are trying to predict.</li> <li>x<sub>i</sub>: the independent variables: the features our model uses to model y.</li> <li>\u03b2<sub>i</sub>\u200b: the coefficients (aka \"weights\") of our regression model. These are the foundations of our model. They are what our model \"learns\" during optimization.[\u2139]</li> <li>\u03f5: the irreducible error in our model. A term that collects together all the unmodeled parts of our data.</li> </ul> <p>Fitting a linear regression model is all about finding the set of cofficients that best model y as a function of our features. We may never know the true parameters for our model, but we can estimate them (more on this later). Once we've estimated these coefficients, \u03b2<sub>i</sub><sup>^</sup> \u200b, we predict future values, y<sup>^</sup>, as:</p> <p>y<sup>^</sup> \u200b= \u03b2<sub>0</sub><sup>^</sup> \u200b\u200b+ \u03b2<sub>1</sub><sup>^</sup>x<sub>1</sub> \u200b+ \u03b2<sub>2</sub><sup>^</sup>x<sub>2</sub> \u200b+ ..... + \u03b2<sub>p</sub><sup>^</sup>x<sup>p</sup></p> <p>So predicting future values (often called inference), is as simple as plugging the values of our features xixi\u200b into our equation!</p>"},{"location":"Day%2011/","title":"Day 11","text":""},{"location":"Day%2011/#how-it-works-briefly","title":"How it works briefly","text":"<p>To make linear regression easier to digest, let's go through a quick, high-level introduction of how it works. We'll scroll through the core concepts of the algorithm at a high-level, and then delve into the details thereafter:</p> <p>Let's fit a model to predict housing price ($) in San Diego, USA using the size of the house (in square-footage):</p> <p>house-price= \u03b2<sub>1</sub><sup>\u200b^</sup> \u2217 sqft + \u03b2<sub>0</sub></p> <p>We'll start with a very simple model, predicting the price of each house to be just the average house price in our dataset, ~$290,000, ignoring the different sizes of each house:</p> <p>house-price = 0 \u2217 sqft + 290000</p> <p></p> <p>Of course we know this model is bad - the model doesn't fit the data well at all. But how can do quantify exactly how bad?</p> <p>To evaluate our model's performance quantitatively, we plot the error of each observation directly. These errors, or residuals, measure the distance between each observation and the predicted value for that observation. We'll make use of these residuals later when we talk about evaluating regression models, but we can clearly see that our model has a lot of error.</p> <p></p> <p>The goal of linear regression is reducing this error such that we find a line/surface that 'best' fits our data. For our simple regression problem, that involves estimating the y-intercept and slope of our model, \u03b2<sub>0</sub><sup>^</sup>\u200b and \u03b2<sub>1</sub><sup>^</sup>\u200b.</p> <p>For our specific problem, the best fit line is shown. There's still error, sure, but the general pattern is captured well. As a result, we can be reasonably confident that if we plug in new values of square-footage, our predicted values of price would be reasonably accurate.</p> <p></p> <p>Once we've fit our model, predicting future values is super easy! We just plug in any x<sub>i</sub>\u200b values into our equation!</p> <p>For our simple model, that means plugging in a value for sqft into our model:</p> <p>sqft Value: 407</p> <p>y<sup>^</sup> \u200b = 756.9 \u2217 407 \u2212 27153.8</p> <p>y<sup>^</sup> = 280905</p> <p>Thus, our model predicts a house that is 407 square-feet will cost $280,905.</p> <p>Now that we have a high-level idea of how linear regression works, let's dive a bit deeper. The remainder of this article will cover how to evaluate regression models, how to find the \"best\" model, how to interpret different forms of regression models, and the assumptions underpinning correct usage of regression models in statistical settings.</p> <p>Let's dive in another day !!</p>"},{"location":"Day%2012/","title":"Day 12","text":""},{"location":"Day%2012/#model-evaluation","title":"Model Evaluation","text":"<p>To train an accurate linear regression model, we need a way to quantify how good (or bad) our model performs. In machine learning, we call such performance-measuring functions loss functions. Several popular loss functions exist for regression problems. To measure our model's performance, we'll use one of the most popular: mean-squared error (MSE). </p>"},{"location":"Day%2012/#mean-squared-error","title":"Mean Squared Error","text":"<p>MSE quantifies how close a predicted value is to the true value, so we'll use it to quantify how close a regression line is to a set of points. MSE works by squaring the distance between each data point and the regression line (the red residuals in the graphs above), summing the squared values, and then dividing by the number of data points:</p> <p></p> <p>The name is quite literal: take the mean of the squared errors. The squaring of errors prevents negative and positive terms from canceling out in the sum, and gives more weight to points further from the regression line, punishing outliers. In practice, we'll fit our regression model to a set training data, and evaluate it's performance using MSE on the test dataset. </p>"},{"location":"Day%2012/#r-squared","title":"R-Squared","text":"<p>Regression models may also be evaluated with the so-called goodness of fit measures, which summarize how well a model fits a set of data. The most popular goodness of fit measure for linear regression is r-squared, a metric that represents the percentage of the variance in y explained by our features x. More specifically, r-squared measures the percentage of variance explained normalized against the baseline variance of our model (which is just the variance of the mean): </p> <p></p> <p>The highest possible value for r-squared is 1, representing a model that captures 100% of the variance. A negative r-squared means that our model is doing worse (capturing less variance) than a flat line through mean of our data would.</p> <p>To build intuition for yourself, try changing the weight and bias terms below to see how the MSE and r-squared change across different model fits by going through this link</p> <p></p> <p>You will often see R-Squared referenced in statistical contexts as a way to assess model fit. </p>"},{"location":"Day%2012/#selecting-an-evaluation-metric","title":"Selecting an Evaluation Metric","text":"<p>Many methods exist for evaluating regression models, each with different concerns around interpretability, theory, and usability. The evaluation metric should reflect whatever it is you actually care about when making predictions. For example, when we use MSE, we are implicitly saying that we think the cost of our prediction error should reflect the quadratic (squared) distance between what we predicted and what is correct. This may work well if we want to punish outliers or if our data is minimized by the mean, but this comes at the cost of interpretability: we output our error in squared units (though this may be fixed with RMSE). If instead we wanted our error to reflect the linear distance between what we predicted and what is correct, or we wanted our data minimized by the median, we could try something like Mean Abosulte Error (MAE). Whatever the case, you should be thinking of your evaluation metric as part of your modeling process, and select the best metric based on the specific concerns of your use-case.</p>"},{"location":"Day%2013/","title":"Day 13","text":""},{"location":"Day%2013/#learning-the-coefficients","title":"Learning the Coefficients","text":"<p>Let's recap what we've learned so far: Linear regression is all about finding a line (or surface) that fits our data well. And as we just saw, this involves selecting the coefficients for our model that minimize our evaluation metric. But how can we best estimate these coefficients? In practice, they're unknown, and selecting them by hand quickly becomes infeasible for regression models with many features. There must be a better way!</p> <p>Luckily for us, several algorithms exist to do just this. We'll discuss two: an iterative solution and a closed-form solution. </p>"},{"location":"Day%2013/#an-iterative-solution","title":"An iterative solution","text":"<p>Gradient descent is an iterative optimization algorithm that estimates some set of coefficients to yield the minimum of a convex function. Put simply: it will find suitable coefficients for our regression model that minimize prediction error (remember, lower MSE equals better model).</p>"},{"location":"Day%2013/#view-the-little-math","title":"View the little math","text":"<p>Gradient descent works as follows. We assume that we have some convex function representing the error of our machine learning algorithm (in our case, MSE). Gradient descent will iteratively update our model's coefficients in the direction of our error functions minimum</p> <p>In our case, our model takes the form:</p> <p>y<sup>^</sup> \u200b= \u03b2<sub>0</sub>\u200b<sup>^</sup> \u200b+ \u03b2<sub>1</sub><sup>\u200b^</sup>\u200bx</p> <p>and our error function takes the form:</p> <p></p> <p>Our goal is to find the coefficients, \u03b2<sub>0</sub>\u200b and \u03b2<sub>1</sub>\u200b, to minimize the error function. To do this, we'll use the gradient, which represents the direction that the function is increasing, and the rate at which it is increasing. Since we want to find the minimum of this function, we can go in the opposite direction of where it's increasing. This is exactly what Gradient Descent does, it works by taking steps in the direction opposite of where our error function is increasing, proportional to the rate of change. To find the coefficients that minimize the function, we first calculate the derivatives of our error function is increasing. To find the coefficients that minimize first, calculate the derivatives of our loss function, MSE: </p> <p></p> <p>Now that we have the gradients for our error function (with respect to each coefficient to be updated), we perform iterative updates:</p> <p></p> <p>Updating these values iteratively will yield coefficients of our model that minimize error.</p> <p>Gradient descent will iteratively identify the coefficients our model needs to fit the data. Let's see an example directly. We'll fit data to our equation y<sup>^</sup> = \u03b2<sub>0</sub><sup>^</sup> + \u03b2<sub>1</sub><sup>^<sup>x<sub>1</sub>\u200b, so gradient descent will learn two coefficients, \u03b2<sub>0</sub> (the intercept) and \u03b2<sub>1</sub>\u200b (the weight). To do so, interact with the plot below. Try dragging the weights and values to create a 'poorly' fit (large error) solution and run gradient descent to see the error iteratively improve. <p>Tweak the parameters with some visualizations by going through the link like as below:</p> <p></p> <p>Although gradient descent is the most popular optimization algorithm in machine learning, it's not perfect! It doesn't work for every loss function, and it may not always find the most optimal set of coefficients for your model. Still, it has many extensions to help solve these issues, and is widely used across machine learning.</p>"},{"location":"Day%2013/#a-closed-form-solution","title":"A Closed Form Solution","text":"<p>We'd be remiss not to mention the Normal Equation, a widely taught method for obtaining estimates for our linear regression coefficients. The Normal Equation is a closed-form solution that allows us to estimate our coefficients directly by minimizing the residual sum of squares (RSS) of our data:</p> <p></p> <p>The RSS should look familiar - it was a key piece in both the MSE and r-squared formulas that represents our model's total squared error: </p> <p></p> <p>The RSS should look familiar - it was a key piece in both the MSE and r-squared formulas that represents our model's total squared error:</p> <p></p> <p>Despite providing a convenient closed-form solution for finding our optimal coefficients, the Normal Equation estimates are often not used in practice, because of the computational complexity required to invert a matrix with too many features. While our two feature example above runs fast (we can run it in the browser!), most machine learning models are more complicated. For this reason, we often just use gradient descent.</p>"},{"location":"Day%2013/#are-our-coefficients-valid","title":"Are our coefficients valid ?","text":"<p>In research publications and statistical software, coefficients of regression models are often presented with associated p-values. These p-values come from traditional null hypothesis statistical tests: t-tests are used to measure whether a given cofficient is significantly different than zero (the null hypothesis that a particular coefficient \u03b2<sub>i</sub>\u200b equals zero), while F tests are used to measure whether any of the terms in a regression model are significantly different from zero. Different opinions exist on the utility of such tests . We don't take a strong stance on this issue, but believe practitioners should always assess the standard error around any parameter estimates for themselves and present them in their research.</p>"},{"location":"Day%2013/#interpreting-regression-models","title":"Interpreting Regression models","text":"<p>One of the most powerful aspects of regression models is their interpretability. However, different forms of regression models require different interpretations. To make this clear, we'll walk through several typical constructs of a regression model, and describe how to interpret each in turn. For all aforementioned models, we interpret the error term as irreducible noise not captured by our model.</p>"},{"location":"Day%2013/#a-binary-feature","title":"A Binary Feature","text":""},{"location":"Day%2013/#a-continuous-feature","title":"A Continuous Feature","text":""},{"location":"Day%2013/#multivariate-regression","title":"Multivariate regression","text":""},{"location":"Day%2013/#regression-with-interaction","title":"Regression with Interaction","text":""},{"location":"Day%2013/#regression-model-assumptions","title":"Regression model assumptions","text":"<p>When teaching regression models, it's common to mention the various assumptions underpinning linear regression. For completion, we'll list some of those assumptions here. However, in the context of machine learning we care most about if the predictions made from our model generalize well to unseen data. We'll use our model if it generalizes well even if it violates statistical assumptions. Still, no treatment of regression is complete without mentioning the assumptions.</p> <ul> <li> <p>Validity: Does the data we're modeling matches to the problem we're actually trying to solve?</p> </li> <li> <p>Representativeness: Is the sample data used to train the regression model representative of the population to which it will be applied?</p> </li> <li> <p>Additivity and Linearity: The deterministic component of a regression model is a linear function of the separate predictors: y= B<sub>0</sub> + B<sub>1</sub>x<sub>1</sub>+...+ B<sub>p</sub>x<sub>p</sub>.</p> </li> <li> <p>Independence of Errors: The errors from our model are independent.</p> </li> <li> <p>Homoscedasticity: The errors from our model have equal variance.</p> </li> <li>Normality of Errors: The errors from our model are normally distributed.</li> </ul>"},{"location":"Day%2013/#when-assumptions-fail","title":"When Assumptions fail ?","text":"<p>What should we do if the assumptions for our regression model aren't met? Don't fret, it's not the end of the world! First, double-check that the assumptions even matter in the first place: if the predictions made from our model generalize well to unseen data, and our task is to create a model that generalizes well, then we're probably fine. If not, figure out which assumption is being violated, and how to address it! This will change depending on the assumption being violated, but in general, one can attempt to extend the model, accompany new data, transform the existing data, or some combination thereof. If a model transformation is unfit, perhaps the application (or research question) can be changed or restricted to better align with the data. In practice, some combination of the above will usually suffice.</p>"},{"location":"Day%2014/","title":"Day 14","text":""},{"location":"Day%2014/#implementing-linear-regression","title":"Implementing Linear Regression","text":""},{"location":"Day%2014/#simple-linear-regression-with-scikit-learn","title":"Simple Linear Regression with Scikit-Learn","text":"<p>We\u2019ll start with the simplest case, which is simple linear regression. There are five basic steps when you\u2019re implementing linear regression:</p> <ul> <li>Import the packages and classes that you need.</li> <li>Provide data to work with, and eventually do appropriate transformations.</li> <li>Create a regression model and fit it with existing data.</li> <li>Check the results of model fitting to know whether the model is satisfactory.</li> <li>Apply the model for predictions.</li> </ul> <p>These steps are more or less general for most of the regression approaches and implementations. Throughout the rest of the tutorial, you\u2019ll learn how to do these steps for several different scenarios.</p>"},{"location":"Day%2014/#step-1-import-packages-and-classes","title":"Step 1: Import Packages and Classes","text":"<p>The first step is to import the package numpy and the class LinearRegression from sklearn.linear_model:</p> <pre><code>import numpy as np\nfrom sklearn.linear_model import LinearRegression\n</code></pre> <p>Now, you have all the functionalities that you need to implement linear regression.</p> <p>The fundamental data type of NumPy is the array type called numpy.ndarray . The rest of this tutorial uses the term array to refer to instances of the type numpy.ndarray .</p> <p>You\u2019ll use the class sklearn.linear_model.LinearRegression to perform linear and polynomial regression and make predictions accordingly.</p>"},{"location":"Day%2014/#step-2-provide-data","title":"Step 2: Provide Data","text":"<p>The second step is defining data to work with. The inputs (regressors, x) and output (response, y) should be arrays or similar objects. This is the simplest way of providing data for regression:</p> <pre><code>x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))\ny = np.array([5, 20, 14, 32, 22, 38])\n</code></pre> <p>Now, you have two arrays: the input, x, and the output, y. You should call .reshape() on x because this array must be two-dimensional, or more precisely, it must have one column and as many rows as necessary. That\u2019s exactly what the argument (-1, 1) of .reshape() specifies.</p> <p>This is how x and y look now:</p> <pre><code>&gt;&gt;&gt; x\narray([[ 5],\n       [15],\n       [25],\n       [35],\n       [45],\n       [55]])\n\n&gt;&gt;&gt; y\narray([ 5, 20, 14, 32, 22, 38])\n</code></pre> <p>As you can see, x has two dimensions, and x.shape is (6, 1), while y has a single dimension, and y.shape is (6,).</p>"},{"location":"Day%2014/#step-3-create-model-and-fit-it","title":"Step 3: Create Model and Fit it","text":"<p>The next step is to create a linear regression model and fit it using the existing data.</p> <p>Create an instance of the class LinearRegression, which will represent the regression model:</p> <pre><code>model = LinearRegression()\n</code></pre> <p>This statement creates the variable model as an instance of LinearRegression. You can provide several optional parameters to LinearRegression:</p> <ul> <li>fit_intercept is a Boolean that, if True, decides to calculate the intercept \ud835\udc4f\u2080 or, if False, considers it equal to zero. It defaults to True.</li> <li>normalize is a Boolean that, if True, decides to normalize the input variables. It defaults to False, in which case it doesn\u2019t normalize the input variables.</li> <li>copy_X is a Boolean that decides whether to copy (True) or overwrite the input variables (False). It\u2019s True by default.</li> <li>n_jobs is either an integer or None. It represents the number of jobs used in parallel computation. It defaults to None, which usually means one job. -1 means to use all available processors.</li> </ul> <p>Your model as defined above uses the default values of all parameters.</p> <p>It\u2019s time to start using the model. First, you need to call <code>.fit()</code> on model:</p> <pre><code>&gt;&gt;&gt; model.fit(x, y)\nLinearRegression()\n</code></pre> <p>With <code>.fit()</code>, you calculate the optimal values of the weights \ud835\udc4f\u2080 and \ud835\udc4f\u2081, using the existing input and output, x and y, as the arguments. In other words, .fit() fits the model. It returns self, which is the variable model itself. That\u2019s why you can replace the last two statements with this one:</p> <pre><code>model = LinearRegression().fit(x, y)\n</code></pre> <p>This statement does the same thing as the previous two. It\u2019s just shorter.</p>"},{"location":"Day%2014/#step-4-get-results","title":"Step 4: Get results","text":"<p>Once you have your model fitted, you can get the results to check whether the model works satisfactorily and to interpret it.</p> <p>You can obtain the coefficient of determination, \ud835\udc45\u00b2, with .score() called on model:</p> <pre><code>&gt;&gt;&gt; r_sq = model.score(x, y)\n&gt;&gt;&gt; print(f\"coefficient of determination: {r_sq}\")\ncoefficient of determination: 0.7158756137479542\n</code></pre> <p>When you\u2019re applying .score(), the arguments are also the predictor x and response y, and the return value is \ud835\udc45\u00b2.</p> <p>The attributes of model are .intercept_, which represents the coefficient \ud835\udc4f\u2080, and .coef_, which represents \ud835\udc4f\u2081:</p> <pre><code>&gt;&gt;&gt; print(f\"intercept: {model.intercept_}\")\nintercept: 5.633333333333329\n\n&gt;&gt;&gt; print(f\"slope: {model.coef_}\")\nslope: [0.54]\n</code></pre> <p>The code above illustrates how to get \ud835\udc4f\u2080 and \ud835\udc4f\u2081. You can notice that .intercept_ is a scalar, while .coef_ is an array.</p> <p>Note: In scikit-learn, by convention, a trailing underscore indicates that an attribute is estimated. In this example, .intercept_ and .coef_ are estimated values.</p> <p>The value of \ud835\udc4f\u2080 is approximately 5.63. This illustrates that your model predicts the response 5.63 when \ud835\udc65 is zero. The value \ud835\udc4f\u2081 = 0.54 means that the predicted response rises by 0.54 when \ud835\udc65 is increased by one.</p> <p>You\u2019ll notice that you can provide y as a two-dimensional array as well. In this case, you\u2019ll get a similar result. This is how it might look:</p> <pre><code>&gt;&gt;&gt; new_model = LinearRegression().fit(x, y.reshape((-1, 1)))\n&gt;&gt;&gt; print(f\"intercept: {new_model.intercept_}\")\nintercept: [5.63333333]\n\n&gt;&gt;&gt; print(f\"slope: {new_model.coef_}\")\nslope: [[0.54]]\n</code></pre> <p>As you can see, this example is very similar to the previous one, but in this case, .intercept_ is a one-dimensional array with the single element \ud835\udc4f\u2080, and .coef_ is a two-dimensional array with the single element \ud835\udc4f\u2081.</p>"},{"location":"Day%2014/#step-5-predict-response","title":"Step 5: Predict response","text":"<p>Once you have a satisfactory model, then you can use it for predictions with either existing or new data. To obtain the predicted response, use <code>.predict()</code>:</p> <pre><code>&gt;&gt;&gt; y_pred = model.predict(x)\n&gt;&gt;&gt; print(f\"predicted response:\\n{y_pred}\")\npredicted response:\n[ 8.33333333 13.73333333 19.13333333 24.53333333 29.93333333 35.33333333]\n</code></pre> <p>When applying .predict(), you pass the regressor as the argument and get the corresponding predicted response. This is a nearly identical way to predict the response:</p> <pre><code>&gt;&gt;&gt; y_pred = model.intercept_ + model.coef_ * x\n&gt;&gt;&gt; print(f\"predicted response:\\n{y_pred}\")\npredicted response:\n[[ 8.33333333]\n [13.73333333]\n [19.13333333]\n [24.53333333]\n [29.93333333]\n [35.33333333]]\n</code></pre> <p>n this case, you multiply each element of x with model.coef_ and add model.intercept_ to the product.</p> <p>The output here differs from the previous example only in dimensions. The predicted response is now a two-dimensional array, while in the previous case, it had one dimension.</p> <p>If you reduce the number of dimensions of x to one, then these two approaches will yield the same result. You can do this by replacing x with x.reshape(-1), x.flatten(), or x.ravel() when multiplying it with model.coef_.</p> <p>In practice, regression models are often applied for forecasts. This means that you can use fitted models to calculate the outputs based on new inputs:</p> <pre><code>&gt;&gt;&gt; x_new = np.arange(5).reshape((-1, 1))\n&gt;&gt;&gt; x_new\narray([[0],\n       [1],\n       [2],\n       [3],\n       [4]])\n\n&gt;&gt;&gt; y_new = model.predict(x_new)\n&gt;&gt;&gt; y_new\narray([5.63333333, 6.17333333, 6.71333333, 7.25333333, 7.79333333])\n</code></pre> <p>Here .predict() is applied to the new regressor x_new and yields the response y_new. This example conveniently uses arange() from numpy to generate an array with the elements from 0, inclusive, up to but excluding 5\u2014that is, 0, 1, 2, 3, and 4.</p>"},{"location":"Day%2014/#multiple-linear-regression-with-scikit-learn","title":"Multiple Linear Regression with Scikit-Learn","text":"<p>You can implement multiple linear regression following the same steps as you would for simple regression. The main difference is that your x array will now have two or more columns.</p>"},{"location":"Day%2014/#steps-1-and-2-import-packages-and-classes-and-provide-data","title":"Steps 1 and 2: Import packages and classes, and provide data","text":"<p>First, you import numpy and sklearn.linear_model.LinearRegression and provide known inputs and output:</p> <pre><code>import numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nx = [\n  [0, 1], [5, 1], [15, 2], [25, 5], [35, 11], [45, 15], [55, 34], [60, 35]\n]\ny = [4, 5, 20, 14, 32, 22, 38, 43]\nx, y = np.array(x), np.array(y)\n</code></pre> <p>That\u2019s a simple way to define the input x and output y. You can print x and y to see how they look now:</p> <pre><code>&gt;&gt;&gt; x\narray([[ 0,  1],\n       [ 5,  1],\n       [15,  2],\n       [25,  5],\n       [35, 11],\n       [45, 15],\n       [55, 34],\n       [60, 35]])\n\n&gt;&gt;&gt; y\narray([ 4,  5, 20, 14, 32, 22, 38, 43])\n</code></pre> <p>In multiple linear regression, x is a two-dimensional array with at least two columns, while y is usually a one-dimensional array. This is a simple example of multiple linear regression, and x has exactly two columns.</p>"},{"location":"Day%2014/#step-3-create-a-model-and-fit-it","title":"Step 3: Create a model and fit it","text":"<p>The next step is to create the regression model as an instance of LinearRegression and fit it with .fit():</p> <pre><code>model = LinearRegression().fit(x, y)\n</code></pre> <p>The result of this statement is the variable model referring to the object of type LinearRegression. It represents the regression model fitted with existing data.</p>"},{"location":"Day%2014/#step-4-get-results_1","title":"Step 4: Get results","text":"<p>You can obtain the properties of the model the same way as in the case of simple linear regression:</p> <pre><code>&gt;&gt;&gt; r_sq = model.score(x, y)\n&gt;&gt;&gt; print(f\"coefficient of determination: {r_sq}\")\ncoefficient of determination: 0.8615939258756776\n\n&gt;&gt;&gt; print(f\"intercept: {model.intercept_}\")\nintercept: 5.52257927519819\n\n&gt;&gt;&gt; print(f\"coefficients: {model.coef_}\")\ncoefficients: [0.44706965 0.25502548]\n</code></pre> <p>You obtain the value of \ud835\udc45\u00b2 using .score() and the values of the estimators of regression coefficients with .intercept_ and .coef_. Again, .intercept_ holds the bias \ud835\udc4f\u2080, while now .coef_ is an array containing \ud835\udc4f\u2081 and \ud835\udc4f\u2082.</p> <p>In this example, the intercept is approximately 5.52, and this is the value of the predicted response when \ud835\udc65\u2081 = \ud835\udc65\u2082 = 0. An increase of \ud835\udc65\u2081 by 1 yields a rise of the predicted response by 0.45. Similarly, when \ud835\udc65\u2082 grows by 1, the response rises by 0.26.</p>"},{"location":"Day%2014/#step-5-predict-response_1","title":"Step 5: Predict response","text":"<p>Predictions also work the same way as in the case of simple linear regression:</p> <pre><code>&gt;&gt;&gt; y_pred = model.predict(x)\n&gt;&gt;&gt; print(f\"predicted response:\\n{y_pred}\")\npredicted response:\n[ 5.77760476  8.012953   12.73867497 17.9744479  23.97529728 29.4660957\n 38.78227633 41.27265006]\n</code></pre> <p>You can predict the output values by multiplying each column of the input with the appropriate weight, summing the results, and adding the intercept to the sum.</p> <p>You can apply this model to new data as well:</p> <pre><code>&gt;&gt;&gt; x_new = np.arange(10).reshape((-1, 2))\n&gt;&gt;&gt; x_new\narray([[0, 1],\n       [2, 3],\n       [4, 5],\n       [6, 7],\n       [8, 9]])\n\n&gt;&gt;&gt; y_new = model.predict(x_new)\n&gt;&gt;&gt; y_new\narray([ 5.77760476,  7.18179502,  8.58598528,  9.99017554, 11.3943658 ])\n</code></pre> <p>That\u2019s the prediction using a linear regression model.</p>"},{"location":"Day%2014/#polynomial-regression-with-scikit-learn","title":"Polynomial Regression with Scikit-Learn","text":"<p>Implementing polynomial regression with scikit-learn is very similar to linear regression. There\u2019s only one extra step: you need to transform the array of inputs to include nonlinear terms such as \ud835\udc65\u00b2.</p>"},{"location":"Day%2014/#step-1-import-packages-and-classes_1","title":"Step 1: Import packages and classes","text":"<p>In addition to numpy and sklearn.linear_model.LinearRegression, you should also import the class <code>PolynomialFeatures from sklearn.preprocessing</code>:</p> <pre><code>import numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n</code></pre> <p>The import is now done, and you have everything you need to work with.</p>"},{"location":"Day%2014/#step-2a-provide-data","title":"Step 2a: Provide Data","text":"<p>This step defines the input and output and is the same as in the case of linear regression:</p> <pre><code>x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))\ny = np.array([15, 11, 2, 8, 25, 32])\n</code></pre> <p>Now you have the input and output in a suitable format. Keep in mind that you need the input to be a two-dimensional array. That\u2019s why .reshape() is used.</p>"},{"location":"Day%2014/#step-2b-transform-input-data","title":"Step 2b: Transform input data","text":"<p>This is the new step that you need to implement for polynomial regression!</p> <p>As you learned earlier, you need to include \ud835\udc65\u00b2\u2014and perhaps other terms\u2014as additional features when implementing polynomial regression. For that reason, you should transform the input array x to contain any additional columns with the values of \ud835\udc65\u00b2, and eventually more features.</p> <p>It\u2019s possible to transform the input array in several ways, like using insert() from numpy. But the class PolynomialFeatures is very convenient for this purpose. Go ahead and create an instance of this class:</p> <pre><code>transformer = PolynomialFeatures(degree=2, include_bias=False)\n</code></pre> <p>The variable transformer refers to an instance of PolynomialFeatures that you can use to transform the input x.</p> <p>You can provide several optional parameters to PolynomialFeatures:</p> <ul> <li>degree is an integer (2 by default) that represents the degree of the polynomial regression function.</li> <li>interaction_only is a Boolean (False by default) that decides whether to include only interaction features (True) or all features (False).</li> <li>include_bias is a Boolean (True by default) that decides whether to include the bias, or intercept, column of 1 values (True) or not (False).</li> </ul> <p>This example uses the default values of all parameters except include_bias. You\u2019ll sometimes want to experiment with the degree of the function, and it can be beneficial for readability to provide this argument anyway.</p> <p>Before applying transformer, you need to fit it with <code>.fit()</code>:</p> <pre><code>&gt;&gt;&gt; transformer.fit(x)\nPolynomialFeatures(include_bias=False)\n</code></pre> <p>Once transformer is fitted, then it\u2019s ready to create a new, modified input array. You apply .transform() to do that:</p> <pre><code>x_ = transformer.transform(x)\n</code></pre> <p>That\u2019s the transformation of the input array with .transform(). It takes the input array as the argument and returns the modified array.</p> <p>You can also use <code>.fit_transform()</code> to replace the three previous statements with only one:</p> <pre><code>x_ = PolynomialFeatures(degree=2, include_bias=False).fit_transform(x)\n</code></pre> <p>With .fit_transform(), you\u2019re fitting and transforming the input array in one statement. This method also takes the input array and effectively does the same thing as .fit() and .transform() called in that order. It also returns the modified array. This is how the new input array looks:</p> <pre><code>&gt;&gt;&gt; x_\narray([[   5.,   25.],\n       [  15.,  225.],\n       [  25.,  625.],\n       [  35., 1225.],\n       [  45., 2025.],\n       [  55., 3025.]])\n</code></pre> <p>The modified input array contains two columns: one with the original inputs and the other with their squares. You can find more information about PolynomialFeatures on the official documentation page.</p>"},{"location":"Day%2014/#step-3-create-a-model-and-fit-it_1","title":"Step 3: Create a model and fit it","text":"<p>This step is also the same as in the case of linear regression. You create and fit the model:</p> <pre><code>model = LinearRegression().fit(x_, y)\n</code></pre> <p>The regression model is now created and fitted. It\u2019s ready for application. You should keep in mind that the first argument of .fit() is the modified input array x_ and not the original x.</p>"},{"location":"Day%2014/#step-4-get-results_2","title":"Step 4: Get results","text":"<p>You can obtain the properties of the model the same way as in the case of linear regression:</p> <pre><code>&gt;&gt;&gt; r_sq = model.score(x_, y)\n&gt;&gt;&gt; print(f\"coefficient of determination: {r_sq}\")\ncoefficient of determination: 0.8908516262498563\n\n&gt;&gt;&gt; print(f\"intercept: {model.intercept_}\")\nintercept: 21.372321428571436\n\n&gt;&gt;&gt; print(f\"coefficients: {model.coef_}\")\ncoefficients: [-1.32357143  0.02839286]\n</code></pre> <p>Again, .score() returns \ud835\udc45\u00b2. Its first argument is also the modified input x_, not x. The values of the weights are associated to .intercept_ and .coef_. Here, .intercept_ represents \ud835\udc4f\u2080, while .coef_ references the array that contains \ud835\udc4f\u2081 and \ud835\udc4f\u2082.</p> <p>You can obtain a very similar result with different transformation and regression arguments:</p> <pre><code>x_ = PolynomialFeatures(degree=2, include_bias=True).fit_transform(x)\n</code></pre> <p>If you call PolynomialFeatures with the default parameter include_bias=True, or if you just omit it, then you\u2019ll obtain the new input array x_ with the additional leftmost column containing only 1 values. This column corresponds to the intercept. This is how the modified input array looks in this case:</p> <pre><code>&gt;&gt;&gt; x_\narray([[1.000e+00, 5.000e+00, 2.500e+01],\n       [1.000e+00, 1.500e+01, 2.250e+02],\n       [1.000e+00, 2.500e+01, 6.250e+02],\n       [1.000e+00, 3.500e+01, 1.225e+03],\n       [1.000e+00, 4.500e+01, 2.025e+03],\n       [1.000e+00, 5.500e+01, 3.025e+03]])\n</code></pre> <p>The first column of x_ contains ones, the second has the values of x, while the third holds the squares of x.</p> <p>The intercept is already included with the leftmost column of ones, and you don\u2019t need to include it again when creating the instance of LinearRegression. Thus, you can provide fit_intercept=False. This is how the next statement looks:</p> <pre><code>model = LinearRegression(fit_intercept=False).fit(x_, y)\n</code></pre> <p>The variable model again corresponds to the new input array x_. Therefore, x_ should be passed as the first argument instead of x.</p> <p>This approach yields the following results, which are similar to the previous case:</p> <pre><code>&gt;&gt;&gt; r_sq = model.score(x_, y)\n&gt;&gt;&gt; print(f\"coefficient of determination: {r_sq}\")\ncoefficient of determination: 0.8908516262498564\n\n&gt;&gt;&gt; print(f\"intercept: {model.intercept_}\")\nintercept: 0.0\n\n&gt;&gt;&gt; print(f\"coefficients: {model.coef_}\")\ncoefficients: [21.37232143 -1.32357143  0.02839286]\n</code></pre> <p>You see that now .intercept_ is zero, but .coef_ actually contains \ud835\udc4f\u2080 as its first element. Everything else is the same.</p>"},{"location":"Day%2014/#step-5-predict-response_2","title":"Step 5: Predict response","text":"<p>If you want to get the predicted response, just use .predict(), but remember that the argument should be the modified input x_ instead of the old x:</p> <pre><code>&gt;&gt;&gt; y_pred = model.predict(x_)\n&gt;&gt;&gt; print(f\"predicted response:\\n{y_pred}\")\npredicted response:\n[15.46428571  7.90714286  6.02857143  9.82857143 19.30714286 34.46428571]\n</code></pre> <p>As you can see, the prediction works almost the same way as in the case of linear regression. It just requires the modified input instead of the original.</p> <p>You can apply an identical procedure if you have several input variables. You\u2019ll have an input array with more than one column, but everything else will be the same. Here\u2019s an example:</p> <pre><code># Step 1: Import packages and classes\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Step 2a: Provide data\nx = [\n  [0, 1], [5, 1], [15, 2], [25, 5], [35, 11], [45, 15], [55, 34], [60, 35]\n]\ny = [4, 5, 20, 14, 32, 22, 38, 43]\nx, y = np.array(x), np.array(y)\n\n# Step 2b: Transform input data\nx_ = PolynomialFeatures(degree=2, include_bias=False).fit_transform(x)\n\n# Step 3: Create a model and fit it\nmodel = LinearRegression().fit(x_, y)\n\n# Step 4: Get results\nr_sq = model.score(x_, y)\nintercept, coefficients = model.intercept_, model.coef_\n\n# Step 5: Predict response\ny_pred = model.predict(x_)\n</code></pre> <p>This regression example yields the following results and predictions:</p> <pre><code>&gt;&gt;&gt; print(f\"coefficient of determination: {r_sq}\")\ncoefficient of determination: 0.9453701449127822\n\n&gt;&gt;&gt; print(f\"intercept: {intercept}\")\nintercept: 0.8430556452395876\n\n&gt;&gt;&gt; print(f\"coefficients:\\n{coefficients}\")\ncoefficients:\n[ 2.44828275  0.16160353 -0.15259677  0.47928683 -0.4641851 ]\n\n&gt;&gt;&gt; print(f\"predicted response:\\n{y_pred}\")\npredicted response:\n[ 0.54047408 11.36340283 16.07809622 15.79139    29.73858619 23.50834636\n 39.05631386 41.92339046]\n</code></pre> <p>In this case, there are six regression coefficients, including the intercept, as shown in the estimated regression function \ud835\udc53(\ud835\udc65\u2081, \ud835\udc65\u2082) = \ud835\udc4f\u2080 + \ud835\udc4f\u2081\ud835\udc65\u2081 + \ud835\udc4f\u2082\ud835\udc65\u2082 + \ud835\udc4f\u2083\ud835\udc65\u2081\u00b2 + \ud835\udc4f\u2084\ud835\udc65\u2081\ud835\udc65\u2082 + \ud835\udc4f\u2085\ud835\udc65\u2082\u00b2.</p> <p>You can also notice that polynomial regression yielded a higher coefficient of determination than multiple linear regression for the same problem. At first, you could think that obtaining such a large \ud835\udc45\u00b2 is an excellent result. It might be.</p> <p>However, in real-world situations, having a complex model and \ud835\udc45\u00b2 very close to one might also be a sign of overfitting. To check the performance of a model, you should test it with new data\u2014that is, with observations not used to fit, or train, the model. To learn how to split your dataset into the training and test subsets, check out Split Your Dataset With scikit-learn\u2019s train_test_split().</p>"},{"location":"Day%2014/#advanced-linear-regression-with-statsmodel","title":"Advanced Linear Regression with Statsmodel","text":""},{"location":"Day%2015/","title":"Day 15","text":""},{"location":"Day%2015/#linear-regression-interview-questions","title":"Linear Regression Interview Questions","text":""},{"location":"Day%2015/#question-1","title":"Question 1","text":""},{"location":"Day%2015/#what-is-linear-regression","title":"What is linear regression?","text":"<p>In simple terms, linear regression is a method of finding the best straight line fitting to the given data, i.e. finding the best linear relationship between the independent and dependent variables.</p> <p>In technical terms, linear regression is a machine learning algorithm that finds the best linear-fit relationship on any given data, between independent and dependent variables. It is mostly done by the Sum of Squared Residuals Method.</p> <p>To know more, click here</p>"},{"location":"Day%2015/#question-2","title":"Question 2","text":""},{"location":"Day%2015/#what-are-the-important-assumptions-of-linear-regression","title":"What are the important assumptions of Linear regression?","text":"<p>Following are the assumptions</p> <ul> <li>A linear Relationship \u2013 Firstly, there has to be a linear relationship between the dependent and the independent variables. To check this relationship, a scatter plot proves to be useful.</li> <li>Restricted Multi-collinearity value \u2013 Secondly, there must no or very little multi-collinearity between the independent variables in the dataset. The value needs to be restricted, which depends on the domain requirement.</li> <li>Homoscedasticity \u2013 The third is the homoscedasticity. It is one of the most important assumptions which states that the errors are equally distributed. </li> </ul>"},{"location":"Day%2015/#question-3","title":"Question 3","text":""},{"location":"Day%2015/#what-is-heteroscedasticity","title":"What is heteroscedasticity?","text":"<p>Heteroscedasticity is exactly the opposite of homoscedasticity, which means that the error terms are not equally distributed. To correct this phenomenon, usually, a log function is used.</p>"},{"location":"Day%2015/#question-4","title":"Question 4","text":""},{"location":"Day%2015/#what-is-the-difference-between-r-square-and-adjusted-r-square","title":"What is the difference between R square and adjusted R square?","text":"<p>R square and adjusted R square values are used for model validation in case of linear regression. R square indicates the variation of all the independent variables on the dependent variable. I.e. it considers all the independent variable to explain the variation. In the case of Adjusted R squared, it considers only significant variables (P values less than 0.05) to indicate the percentage of variation in the model. To know more, click here</p>"},{"location":"Day%2015/#question-5","title":"Question 5","text":""},{"location":"Day%2015/#can-we-use-linear-regression-for-time-series-analysis","title":"Can we use linear regression for time series analysis?","text":"<p>One can use linear regression for time series analysis, but the results are not promising. So, it is generally not advisable to do so. The reasons behind this are.</p> <ol> <li>Time series data is mostly used for the prediction of the future, but linear regression seldom gives good results for future prediction as it is not meant for extrapolation.</li> <li>Mostly, time series data have a pattern, such as during peak hours, festive seasons, etc., which would most likely be treated as outliers in the linear regression analysis</li> </ol>"},{"location":"Day%2015/#question-6","title":"Question 6","text":""},{"location":"Day%2015/#what-is-vif-how-do-you-calculate-it","title":"What is VIF? How do you calculate it?","text":"<p>Variance Inflation Factor (VIF) is used to check the presence of multicollinearity in a data set. It is calculated as</p> <p></p> <p>Here, VIFj  is the value of VIF for the jth variable, Rj2 is the R2 value of the model when that variable is regressed against all the other independent variables.</p> <p>If the value of VIF is high for a variable, it implies that the R2  value of the corresponding model is high, i.e. other independent variables are able to explain that variable. In simple terms, the variable is linearly dependent on some other variables.</p>"},{"location":"Day%2015/#question-7","title":"Question 7","text":""},{"location":"Day%2015/#how-to-find-rmse-and-mse","title":"How to find RMSE and MSE?","text":"<p>RMSE and MSE are the two of the most common measures of accuracy for a linear regression.</p> <p>RMSE indicates the Root mean square error, which indicated by the formula:</p> <p></p> <p>Where MSE indicates the Mean square error represented by the formula:</p> <p></p>"},{"location":"Day%2015/#question-8","title":"Question 8","text":""},{"location":"Day%2015/#how-to-interpret-a-q-q-plot-in-a-linear-regression-model","title":"How to interpret a Q-Q plot in a Linear regression model?","text":"<p>A Q-Q plot is used to check the normality of errors. In the above chart mentioned, Majority of the data follows a normal distribution with tails curled. This shows that the errors are mostly normally distributed but some observations may be due to significantly higher/lower values are affecting the normality of errors.</p>"},{"location":"Day%2015/#question-9","title":"Question 9","text":""},{"location":"Day%2015/#what-is-the-significance-of-an-f-test-in-a-linear-model","title":"What is the significance of an F-test in a linear model?","text":"<p>The use of F-test is to test the goodness of the model. When the model is re-iterated to improve the accuracy with changes, the F-test values prove to be useful in terms of understanding the effect of overall regression.</p>"},{"location":"Day%2015/#question-10","title":"Question 10","text":""},{"location":"Day%2015/#what-are-the-disadvantages-of-the-linear-model","title":"What are the disadvantages of the linear model?","text":"<p>Linear regression is sensitive to outliers which may affect the result.</p> <p>\u2013 Over-fitting</p> <p>\u2013 Under-fitting</p>"},{"location":"Day%2015/#question-11","title":"Question 11","text":""},{"location":"Day%2015/#you-run-your-regression-on-different-subsets-of-your-data-and-in-each-subset-the-beta-value-for-a-certain-variable-varies-wildly-what-could-be-the-issue-here","title":"You run your regression on different subsets of your data, and in each subset, the beta value for a certain variable varies wildly. What could be the issue here?","text":"<p>This case implies that the dataset is heterogeneous. So, to overcome this problem, the dataset should be clustered into different subsets, and then separate models should be built for each cluster. Another way to deal with this problem is to use non-parametric models, such as decision trees, which can deal with heterogeneous data quite efficiently.</p>"},{"location":"Day%2015/#question-12","title":"Question 12","text":""},{"location":"Day%2015/#which-graphs-are-suggested-to-be-observed-before-model-fitting","title":"Which graphs are suggested to be observed before model fitting?","text":"<p>Before fitting the model, one must be well aware of the data, such as what the trends, distribution, skewness, etc. in the variables are. Graphs such as histograms, box plots, and dot plots can be used to observe the distribution of the variables. Apart from this, one must also analyse what the relationship between dependent and independent variables is. This can be done by scatter plots (in case of univariate problems), rotating plots, dynamic plots, etc .</p>"},{"location":"Day%2015/#question-13","title":"Question 13","text":""},{"location":"Day%2015/#explain-the-bias-variance-trade-off","title":"Explain the bias-variance trade-off.","text":"<p>Bias refers to the difference between the values predicted by the model and the real values. It is an error. One of the goals of an ML algorithm is to have a low bias. Variance refers to the sensitivity of the model to small fluctuations in the training dataset. Another goal of an ML algorithm is to have low variance. For a dataset that is not exactly linear, it is not possible to have both bias and variance low at the same time. A straight line model will have low variance but high bias, whereas a high-degree polynomial will have low bias but high variance.</p> <p>There is no escaping the relationship between bias and variance in machine learning.</p> <ul> <li>Decreasing the bias increases the variance.</li> <li>Decreasing the variance increases the bias.</li> </ul> <p>So, there is a trade-off between the two; the ML specialist has to decide, based on the assigned problem, how much bias and variance can be tolerated. Based on this, the final model is built.</p>"},{"location":"Day%2015/#question-14","title":"Question 14","text":""},{"location":"Day%2015/#what-is-mae-and-rmse-and-what-is-the-difference-between-the-matrices","title":"What is MAE and RMSE and what is the difference between the matrices?","text":"<p>Mean Absolute Error (MAE): MAE measures the average magnitude of the errors in a set of predictions, without considering their direction. It\u2019s the average over the test sample of the absolute differences between prediction and actual observation where all individual differences have equal weight.</p> <p></p> <p>Root mean squared error (RMSE): RMSE is a quadratic scoring rule that also measures the average magnitude of the error. It\u2019s the square root of the average of squared differences between prediction and actual observation.</p> <p></p> <p>Difference :</p> <ul> <li> <p>Taking the square root of the average squared errors has some interesting implications for RMSE. Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means the RMSE should be more useful when large errors are particularly undesirable.</p> </li> <li> <p>From an interpretation standpoint, MAE is clearly the winner. RMSE does not describe average error alone and has other implications that are more difficult to tease out and understand.</p> </li> <li> <p>On the other hand, one distinct advantage of RMSE over MAE is that RMSE avoids the use of taking the absolute value, which is undesirable in many mathematical calculations.</p> </li> </ul>"},{"location":"Day%2016/","title":"Day 16","text":""},{"location":"Day%2016/#what-is-logistic-regression","title":"What is Logistic Regression ?","text":"<p>One major area in machine learning is supervised learning, where the goal is to predict an output given some inputs. The output value may be a numerical or categorical variable. In this article, we will discuss logistic regression: a supervised learning algorithm that can be used to classify data into categories, or classes, by predicting the probability that an observation falls into a particular class based on its features.</p> <p>Though it can be extended to more than two categories, logistic regression is often used for binary classification, i.e. determining which of two groups a data point belongs to, or whether an event will occur or not. In this article, we will focus on binary logistic regression.</p> <p>The typical setup for logistic regression is as follows: there is an outcome yy that falls into one of two categories (say 0 or 1), and the following equation is used to estimate the probability that y belongs to a particular category given inputs X=(x<sub>1</sub>,x<sub>2</sub>,...,x<sub>k</sub>): </p> <p>P(y = 1 \u2223 X) = sigmoid(z) = 1/1 + e<sup>\u2212z</sup></p> <p>If you have ever encountered linear regression, the equation for z likely looks familiar. This is called a linear predictor, and it is transformed by the sigmoid function so that the values fall between 0 and 1, and can therefore be interpreted as probabilities. This resulting probability is then compared to a threshold to predict a class for y based on X.\u200b\u200b</p>"},{"location":"Day%2016/#how-it-works","title":"How it Works ?","text":"<p>Let\u2019s make this a bit more concrete by walking through an example. Suppose that you want to go for a hike in Seattle. You want to predict whether it will be sunny or rainy, so that you can decide whether to hike or drink coffee indoors at a local cafe. You know that it rains often in Seattle, but you\u2019ve heard the summers have nice weather. The question is: can we predict the weather, given factors such as the temperature?</p> <p></p> <p></p> <p></p> <p>Now to visualize , how the threshold affects the sigmoid function, click here</p> <p></p>"},{"location":"Day%2017/","title":"Day 17","text":""},{"location":"Day%2017/#evaluating-our-model","title":"Evaluating Our Model","text":"<p>When fitting our model, the goal is to find the parameters that optimize a function that defines how well the model is performing. Put simply, the goal is to make predictions as close to 1 when the outcome is 1 and as close to 0 when the outcome is 0. In machine learning, the function to be optimized is called the loss function or cost function. We use the loss function to determine how well our model fits the data.</p> <p>A suitable loss function in logistic regression is called the Log-Loss, or binary cross-entropy. This function is: </p> <p></p> <p>where nn is the number of samples, indexed by ii, yiyi\u200b is the true class for the index ii, and pipi\u200b is the model prediction for the index ii. Minimizing the Log-Loss is equivalent to maximizing the Log-Likelihood, since the Log-Loss is the negative of the Log-Likelihood.</p> <p>This graph shows how the Log-Loss depends on the true value for yy and the predicted probability. You can see how as the probability gets closer to the true value (p = 0 when y=0 and p=1 when y=1), the Log-Loss decreases to 0. As the probability gets further from the true value, the Log-Loss approaches infinity.</p> <p></p> <p>Here, true value of y = 0 and probability is 0.5</p> <p>You can visualize more value by going here .</p>"},{"location":"Day%2018/","title":"Day 18","text":""},{"location":"Day%2018/#estimating-coefficients","title":"Estimating Coefficients","text":"<p>How do we find the coefficients \u03b2<sub>0</sub><sup>^</sup>,\u03b2<sub>1</sub><sup>^</sup>,...,\u03b2<sub>k<sub><sup>^</sup>\u200b that minimize the loss function? There are two main approaches for logistic regression : gradient descent and maximum likelihood estimation. We\u2019ll briefly discuss both here."},{"location":"Day%2018/#gradient-descent","title":"Gradient Descent","text":"<p>A common way to estimate coefficients is to use gradient descent. In gradient descent, the goal is to minimize the Log-Loss cost function over all samples. This method involves selecting initial parameter values, and then updating them incrementally by moving them in the direction that decreases the loss. At each iteration, the parameter value is updated by the gradient, scaled by the step size (otherwise known as the learning rate). The gradient is the vector encompassing the direction and rate of the fastest increase of a function, which can be calculated using partial derivatives. The parameters are updated in the opposite direction of the gradient by the step size in an attempt to find the parameter values that minimize the Log-Loss.</p> <p>Because the gradient calculates where the function is increasing, going in the opposite direction leads us to the minimum of our function. In this manner, we can repeatedly update our model's coefficients such that we eventually reach the minimum of our error function and obtain a sigmoid curve that fits our data well.</p> <p></p>"},{"location":"Day%2018/#maximum-likelihood-estimation","title":"Maximum Likelihood Estimation","text":"<p>Another approach is finding the model that maximizes the likelihood of observing the data by using Maximum Likelihood Estimation (MLE). It turns out, minimizing the Log-Loss is equivalent to maximizing the Log-Likelihood. Therefore, the goal is to find the parameter values that maximize the following: </p> <p></p> <p>We can do so by differentiating the Log-Likelihood with respect to the parameters, setting the derivatives equal to 0, and solving the equation to find the estimates of the parameters. Play with parameters here</p>"},{"location":"Day%2019/","title":"Day 19","text":""},{"location":"Day%2019/#interpreting-logistic-regression-models","title":"Interpreting Logistic Regression Models","text":"<p>Interpreting the coefficients of a logistic regression model can be tricky because the coefficients in a logistic regression are on the log-odds scale. This means the interpretations are different than in linear regression.</p> <p>To understand log-odds, we must first understand odds. Odds are calculated as p/1-p. This gives the ratio of the probability of a sunny day to that of a rainy day. Say that the probability of a sunny day is 0.75. This implies that the probability of a rainy day is 0.25. The odds would then be  0.75/0.25 = 3 which means that the odds of a sunny day are 3 to 1. If the probability of rain is 0.5, then the odds would be  0.5/0.5 = 1, meaning that the odds of a sunny day are 1 to 1, so sun and rain are equally likely. Taking the log of the odds yields the log-odds, and taking ratios of log-odds yields log-odds ratios.</p> <p>Now that we understand log-odds a bit more, we can discuss how to interpret the models. Since the coefficients are on the log-odds scale, we can transform them to the odds scale by exponentiating so that they are easier to interpret.</p>"},{"location":"Day%2019/#a-binary-feature","title":"A Binary Feature","text":""},{"location":"Day%2019/#a-continuous-feature","title":"A Continuous Feature","text":""},{"location":"Day%2019/#a-multivariate-feature","title":"A Multivariate Feature","text":""},{"location":"Day%202/","title":"Day 2","text":""},{"location":"Day%202/#machine-learning-interview-questions","title":"Machine Learning Interview Questions \u2753","text":""},{"location":"Day%202/#difference-between-supervised-unsupervised-and-reinforcement-machine-learning","title":"Difference between Supervised, Unsupervised and Reinforcement Machine Learning ?","text":"<p>Certainly! Here are the descriptions of supervised learning, unsupervised learning, and reinforcement learning with emojis:</p> <ol> <li> <p>Supervised Learning:</p> <ul> <li> <p>Objective \ud83c\udf93: In supervised learning, the model is trained on a labeled dataset, where each input data point is associated with a corresponding target or output. The goal is to learn a mapping from inputs to outputs based on the provided labels.</p> </li> <li> <p>Examples \ud83d\udcf8: Image classification \ud83d\uddbc\ufe0f, where the algorithm learns to recognize objects in images; Spam email detection \ud83d\udce7, where the model learns to classify emails as spam or not based on labeled examples; and Regression \ud83d\udcc8, where the model predicts a continuous value, such as predicting house prices \ud83c\udfe1 based on features like square footage and location.</p> </li> <li> <p>Key Characteristics \ud83d\udcda:</p> <ul> <li>The model learns from a teacher or supervisor \ud83d\udc69\u200d\ud83c\udfeb, as it has access to the correct answers during training.</li> <li>It's used for tasks where the goal is to make predictions \ud83d\udcca or classify data into predefined categories \ud83d\uddc2\ufe0f.</li> <li>Supervised learning models are evaluated using metrics like accuracy \u2705, precision \u27a1\ufe0f, recall \u2b05\ufe0f, and mean squared error \ud83d\udccf.</li> </ul> </li> </ul> </li> <li> <p>Unsupervised Learning:</p> <ul> <li> <p>Objective \ud83d\udd75\ufe0f\u200d\u2642\ufe0f: Unsupervised learning deals with unlabeled data \ud83d\udd76\ufe0f, where there are no explicit target labels. Instead, the algorithm aims to discover patterns, structures, or relationships within the data on its own.</p> </li> <li> <p>Examples \ud83e\udde9: Clustering \ud83c\udf10, where data points are grouped into clusters \ud83d\udd35\ud83d\udd34 based on similarities; Dimensionality reduction \ud83d\udd0d, which reduces the number of features while preserving meaningful information; and Anomaly detection \ud83d\udea8, identifying unusual data points \ud83e\udde9 in a dataset.</p> </li> <li> <p>Key Characteristics \ud83d\udd0d:</p> <ul> <li>The model learns without supervision or guidance \ud83e\udd16\ud83d\ude80, making it suitable for exploratory data analysis and finding hidden patterns.</li> <li>It's often used when the goal is to uncover insights \ud83d\udca1, reduce data complexity \ud83e\uddf9, or identify anomalies \u2753.</li> <li>Evaluation can be more challenging \ud83e\udd14, as there are no explicit target labels. It often relies on internal measures like intra-cluster distance \ud83d\udcca or visual inspection \ud83d\udc40.</li> </ul> </li> </ul> </li> <li> <p>Reinforcement Learning:</p> <ul> <li> <p>Objective \ud83e\udd16\ud83d\udd79\ufe0f: Reinforcement learning (RL) involves an agent \ud83e\udd16 that interacts with an environment \ud83c\udfde\ufe0f and learns to make sequential decisions \ud83c\udfae to maximize a cumulative reward \ud83c\udfc6. The agent takes actions \ud83d\udd79\ufe0f, receives feedback (rewards \ud83c\udf1f or penalties \ud83d\udeab), and learns to optimize its actions over time.</p> </li> <li> <p>Examples \ud83e\udd16\ud83c\udfae: Game playing \ud83c\udfae (e.g., AlphaGo, where the AI learned to play the board game Go); Autonomous robotics \ud83e\udd16 (teaching a robot to perform tasks like walking \ud83d\udeb6\u200d\u2642\ufe0f or navigating \ud83d\ude97); and Recommendation systems \ud83d\udcda (learning to recommend products or content \ud83d\udcfa to users while maximizing user engagement \ud83d\udc4d).</p> </li> <li> <p>Key Characteristics \ud83e\udd2f:</p> <ul> <li>The model learns through trial and error \ud83d\udd04, exploring different actions \ud83e\udded and learning from the consequences \u2696\ufe0f.</li> <li>It's used for tasks where the optimal sequence of actions \ud83c\udfc1 is not known in advance, and the agent must learn to make decisions \ud83e\udd14 to achieve long-term goals \ud83c\udf1f.</li> <li>Evaluation typically involves measuring the agent's ability to maximize cumulative rewards over time \u231b.</li> </ul> </li> </ul> </li> </ol>"},{"location":"Day%202/#difference-between-data-engineer-data-scientist-data-analyst-and-machine-learning-engineer","title":"Difference between Data Engineer, Data Scientist, Data Analyst and Machine Learning Engineer ?","text":"<p>Data Engineer, Data Scientist, Data Analyst, and Machine Learning Engineer are distinct roles within the field of data science and machine learning, each with its own set of responsibilities and skill sets. Here's a summary of the key differences between these roles:</p> <ol> <li> <p>Data Engineer:</p> <ul> <li>Responsibilities: Data Engineers are primarily responsible for designing, building, and maintaining the infrastructure and architecture needed for data generation, storage, and retrieval. They ensure data pipelines are efficient, reliable, and scalable.</li> <li>Skills: Proficiency in data warehousing, ETL (Extract, Transform, Load) processes, databases (SQL and NoSQL), big data technologies (e.g., Hadoop, Spark), and data modeling. Knowledge of cloud platforms like AWS, Azure, or GCP is often required.</li> <li>Goal: Data Engineers focus on making data accessible and available for analysis by Data Scientists and Analysts. They ensure data quality, data governance, and data security.</li> </ul> </li> <li> <p>Data Scientist:</p> <ul> <li>Responsibilities: Data Scientists use data to extract insights, build predictive models, and solve complex business problems. They identify patterns, perform statistical analyses, and create machine learning models to make data-driven decisions.</li> <li>Skills: Strong expertise in statistics, data analysis, machine learning, programming (often in Python or R), and data visualization. Domain knowledge and communication skills are also important for translating findings into actionable insights.</li> <li>Goal: Data Scientists aim to generate valuable insights, make predictions, and create data-driven solutions to business challenges.</li> </ul> </li> <li> <p>Data Analyst:</p> <ul> <li>Responsibilities: Data Analysts focus on exploring and interpreting data to answer specific questions or provide insights. They perform descriptive analytics, create reports, and often work with visualization tools to communicate findings.</li> <li>Skills: Proficiency in SQL, data querying, data cleaning, data visualization (using tools like Tableau or Power BI), and domain-specific knowledge. Strong communication skills are essential for presenting findings to non-technical stakeholders.</li> <li>Goal: Data Analysts aim to provide actionable insights and help organizations make informed decisions based on historical data.</li> </ul> </li> <li> <p>Machine Learning Engineer:</p> <ul> <li>Responsibilities: Machine Learning Engineers specialize in developing and deploying machine learning models into production. They work on scaling and optimizing algorithms for real-world applications, often collaborating with Data Scientists to put models into action.</li> <li>Skills: Proficiency in machine learning libraries (e.g., TensorFlow, PyTorch), software engineering, deployment, and containerization (e.g., Docker, Kubernetes). Knowledge of cloud services and DevOps practices is crucial.</li> <li>Goal: Machine Learning Engineers focus on taking models from research and experimentation to practical applications that can be integrated into software systems or products.</li> </ul> </li> </ol> <p>In summary, Data Engineers build and maintain data infrastructure, Data Scientists derive insights and build predictive models, Data Analysts focus on data exploration and reporting, and Machine Learning Engineers specialize in deploying machine learning models into production. These roles often collaborate closely to harness the power of data for business value.</p>"},{"location":"Day%202/#what-is-online-and-offline-learning","title":"What is Online and Offline learning ?","text":"<ol> <li> <p>Offline Machine Learning (Batch Learning) \ud83d\udce6:</p> <ul> <li> <p>Training and Inference \ud83d\ude82\ud83d\udd0d: In offline machine learning, the model is trained on a static dataset that is collected and prepared beforehand. Training occurs in a batch mode, where the entire dataset is processed at once to update the model parameters. Once trained, the model is typically used for inference on new, unseen data.</p> </li> <li> <p>Use Cases \ud83e\uddd0: Offline learning is suitable for scenarios where data collection and model training can be decoupled in time. It is common in applications where the data doesn't change rapidly or where regular, periodic model updates are sufficient.</p> </li> <li> <p>Examples \ud83d\uddbc\ufe0f\ud83d\udce7: Image classification, spam email detection, and offline recommendation systems.</p> </li> <li> <p>Advantages \ud83d\udc4d:</p> <ul> <li>Simplicity in implementation and training.</li> <li>Well-suited for static or slowly changing data.</li> </ul> </li> <li> <p>Disadvantages \ud83d\udc4e:</p> <ul> <li>Not suitable for real-time or rapidly changing data.</li> <li>Model may become stale or less accurate as new data arrives.</li> </ul> </li> </ul> </li> <li> <p>Online Machine Learning (Incremental Learning) \ud83d\udd04:</p> <ul> <li> <p>Training and Inference \ud83c\udfc3\u200d\u2642\ufe0f\ud83c\udfaf: In online machine learning, the model is updated continuously as new data becomes available. It adapts to changing data patterns over time without retraining the entire model. The model can make predictions or decisions in real-time.</p> </li> <li> <p>Use Cases \ud83c\udf10\ud83d\ude80: Online learning is beneficial when the data is generated or changes rapidly, and immediate model updates are required to maintain accuracy. It is commonly used in dynamic, evolving environments.</p> </li> <li> <p>Examples \ud83d\udd75\ufe0f\u200d\u2642\ufe0f\ud83d\ude97: Fraud detection in financial transactions, real-time recommendation systems, and autonomous vehicles.</p> </li> <li> <p>Advantages \ud83d\udc4d:</p> <ul> <li>Suitable for real-time or rapidly changing data.</li> <li>Allows the model to adapt to evolving patterns.</li> <li>Reduces the need for periodic retraining.</li> </ul> </li> <li> <p>Disadvantages \ud83d\udc4e:</p> <ul> <li>Can be more complex to implement due to continuous updates.</li> <li>May require careful handling of drift and concept changes in the data.</li> </ul> </li> </ul> </li> </ol>"},{"location":"Day%202/#how-machine-learning-is-different-from-deep-learning","title":"How Machine Learning is different from Deep Learning ?","text":"Aspect Machine Learning (ML) Deep Learning (DL) Scope \ud83c\udf10 Broader, encompasses various techniques and algorithms. \ud83e\udde0 Subset of ML, focuses on deep neural networks. Representation \ud83d\udcca Relies on handcrafted features, often requires feature engineering. \ud83e\udd16 Learns feature representations automatically from data. Architecture \ud83c\udfe2 Shallow architectures with few layers. \ud83c\udfe2\ud83c\udfe2\ud83c\udfe2 Deep architectures with multiple hidden layers. Training \ud83d\ude82 Optimization techniques like gradient descent. \ud83d\udcbb Computationally intensive, often requires large datasets. Applications \ud83d\udcc8 Widely used in various domains for classification, regression, clustering, etc. \ud83d\udcf7 Excels in unstructured data tasks like image and speech recognition. Interpretability \ud83e\uddd0 Models are often more interpretable as features are designed by humans. \ud83d\udd75\ufe0f\u200d\u2642\ufe0f Can be less interpretable due to complex, automatically learned features."},{"location":"Day%203/","title":"Day 3","text":""},{"location":"Day%203/#machine-learning-development-life-cycle-mldlc","title":"Machine Learning Development Life Cycle (MLDLC) \u2699\ufe0f","text":"<p>The MLDLC is a framework that guides the process of building and deploying machine learning models. It is an iterative process, meaning that the steps are not always linear and may need to be repeated as new information is learned.</p>"},{"location":"Day%203/#steps","title":"Steps:","text":""},{"location":"Day%203/#planning","title":"Planning \ud83c\udfaf","text":"<ul> <li>Define the problem that the machine learning model is intended to solve.</li> <li>Identify the data that will be needed to train the model.</li> <li>Identify the resources that will be available.</li> <li><code>For example</code>, you are working on house price prediction so you need to know ,at which area you are going to predict, how you getting that data (from kaggle or some APIs etc). How you deal with that problem.</li> </ul>"},{"location":"Day%203/#data-collection","title":"Data Collection \ud83d\udc5b","text":"<ul> <li>After framing / planning the problem, we should start collecting the data from API, kaggle or other methods.</li> </ul>"},{"location":"Day%203/#exploratory-data-analysis","title":"Exploratory Data Analysis \ud83c\udfab","text":"<ul> <li>After collecting data, our data is not much cleaner, it is just random facts in a real world scenario.</li> <li>We should most of our time in Exploratory data analysis and data preparation to make machine learning model more accurate.</li> <li>We should analyze each feature and analyze the relationship between every features and to understand on which feature we should work only.</li> <li>Analyze how many missing values are present, are there any outliers etc.</li> </ul>"},{"location":"Day%203/#data-preparation","title":"Data preparation \ud83e\uddf9","text":"<ul> <li>Clean, and prepare the data.</li> <li>Remove outliers.</li> <li>Fill in missing values.</li> <li>Convert the data into a compatible format.</li> </ul>"},{"location":"Day%203/#model-engineering-training-selection","title":"Model engineering (Training &amp; Selection) \ud83d\udd28","text":"<ul> <li>Select and train a machine learning algorithm.</li> <li>Compare different algorithms.</li> <li>Choose the one that is best suited for your needs.</li> </ul>"},{"location":"Day%203/#model-evaluation","title":"Model evaluation \ud83e\uddea","text":"<ul> <li>Evaluate the model's performance on a held-out test set using different performance metrics (we'll study in further days).</li> <li>Identify any areas where the model needs improvement.</li> </ul>"},{"location":"Day%203/#model-deployment","title":"Model deployment \ud83d\ude80","text":"<ul> <li>Integrate the model into an existing software application so that user can interact with our machine learning model.</li> <li>Develop a new application specifically for the model.</li> </ul>"},{"location":"Day%203/#model-monitoring","title":"Model monitoring \ud83d\udd75\ufe0f","text":"<ul> <li>Monitor the model's performance and store the new data day by day.</li> <li>Make adjustments in model and retrain it as new data appears so it is familiar with new usedcases as well.</li> </ul>"},{"location":"Day%203/#tips","title":"Tips","text":"<ul> <li>Start with a clear understanding of the problem. \ud83d\udca1</li> <li>Use high-quality data. \ud83d\udc8e</li> <li>Choose the right algorithm. \ud83c\udfaf</li> <li>Evaluate your model carefully. \ud83e\uddea</li> <li>Deploy your model in a production environment. \ud83d\ude80</li> <li>Monitor your model's performance. \ud83d\udd75\ufe0f</li> </ul>"},{"location":"Day%203/#conclusion","title":"Conclusion","text":"<p>By following these tips, you can increase your chances of building and deploying successful machine learning models. \ud83d\udcaa</p>"},{"location":"Day%204/","title":"Day 4","text":""},{"location":"Day%204/#frame-the-problem","title":"Frame the problem \ud83d\uddbc\ufe0f","text":"<p>Let's start with a example dataset and analyze , extract all the insights that what that data is speaking to us. Hence we are going to get the dataset from kaggle and perform exploratory data analysis.</p>"},{"location":"Day%204/#about-data","title":"About Data \ud83d\uddc4\ufe0f","text":"<p>We are using cars4u dataset which you can access it from here. This dataset is in xls format. Dataset contains many features such as <code>Name</code>, <code>Location</code>, <code>Year</code>, <code>Kilometers_Driven</code>, <code>Fuel_Type</code>, <code>Transmission</code>....... and one target column is <code>price</code> of car.</p>"},{"location":"Day%204/#import-the-essentials","title":"Import the essentials","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#to ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n</code></pre>"},{"location":"Day%204/#view-the-data","title":"View the data \ud83d\udc41\ufe0f\u200d\ud83d\udde8\ufe0f","text":"<p>We can view the sample data using below code -</p> <p><pre><code>data.head()\n</code></pre> Output</p> <p></p>"},{"location":"Day%204/#info-and-describe","title":"Info and describe \ud83e\udd21","text":"<p>As we can see, data contains numerical features as well as categorical features, let's view it and also understand <code>max</code> , <code>min</code>, <code>count</code>, <code>mean</code> etc of all the features for statistical analysis.</p> <p><pre><code>data.info()\n</code></pre> Output</p> <p></p> <pre><code>data.describe().T\n</code></pre> <p>Output</p> <p></p> <p>Note - Describe function analyzes the statistical insights of only numerical features.</p> <p>From the statistics summary, we can infer the below findings :</p> <ul> <li>Years range from 1996- 2019 and has a high in a range which shows used cars contain both latest models and old model cars.</li> <li>On average of Kilometers-driven in Used cars are ~58k KM. The range shows a huge difference between min and max as max values show 650000 KM shows the evidence of an outlier. This record can be removed.</li> <li>Min value of Mileage shows 0 cars won\u2019t be sold with 0 mileage. This sounds like a data entry issue.</li> <li>It looks like Engine and Power have outliers, and the data is right-skewed.</li> <li>The average number of seats in a car is 5. car seat is an important feature in price contribution.</li> <li>The max price of a used car is 160k which is quite weird, such a high price for used cars. There may be an outlier or data entry issue.</li> </ul>"},{"location":"Day%204/#number-of-unique-values","title":"Number of Unique values \uf79c","text":"<p>Why we need to find unique values ?  - because when length of data is not equal to total unique value, it means data contains duplicate values and duplicate values is the senseless thing in the data.</p> <p><pre><code>data.nunique()\n</code></pre> Output</p> <p></p>"},{"location":"Day%204/#missing-values-calculation","title":"Missing values calculation \ud83d\udda9","text":"<p>Our machine learning model doesn't work wells when there is missing values in the data. Sometimes it also gives errors that data has <code>NaN</code> values. So to count number of missing values. we can use </p> <pre><code>data.isnull().sum() # it will display number of missing values in each feature\n</code></pre> <p>Output</p> <p></p> <p>We can see that there are missing values in feature <code>engine</code>, <code>power</code>, <code>seats</code>, <code>new price</code> and <code>price</code>. In further days, we'll deal with those missing values that how we can remove them or replace them.</p>"},{"location":"Day%204/#remove-unwanted-columns","title":"Remove Unwanted columns \ud83d\udcce","text":"<p>There are many columns which we don't want to use for training our machine learning model, like <code>S.No</code> because this column don't have a meaning it is just numbering. So we'll remove them </p> <pre><code>data = data.drop(['S.No.'],axis=1) # axis = 1 is for column wise\n</code></pre> <p>After removing column, dataset will look like</p> <p></p>"},{"location":"Day%204/#feature-engineering","title":"Feature Engineering \ud83c\udfce\ufe0f","text":"<p>Feature engineering refers to the process of using domain knowledge to select and transform the most relevant variables from raw data when creating a predictive model using machine learning or statistical modeling. The main goal of Feature engineering is to create meaningful data from raw data. </p>"},{"location":"Day%204/#creating-features","title":"Creating features \ud83c\udfd7\ufe0f","text":"<p>It would be difficult to find the car\u2019s age if it is in year format as the Age of the car is a contributing factor to Car Price. </p> <p>Let's introduce a new column <code>car_age</code> for calculating age of car.</p> <pre><code>from datetime import date\ndate.today().year\ndata['Car_Age']=date.today().year-data['Year']\ndata.head()\n</code></pre> <p>Output</p> <p></p> <p>Since car names will not be great predictors of the price in our current data. But we can process this column to extract important information using brand and Model names. Let\u2019s split the name and introduce new variables Brand and Model</p> <pre><code>data['Brand'] = data.Name.str.split().str.get(0)\ndata['Model'] = data.Name.str.split().str.get(1) + data.Name.str.split().str.get(2)\ndata[['Name','Brand','Model']]\n</code></pre> <p>After splitting the <code>brand</code>, <code>name</code> and <code>model</code>, columns will look like</p> <p></p>"},{"location":"Day%204/#lets-clean-the-data","title":"Let's clean the data \ud83e\udea5","text":"<p>Some names of the variables are not relevant and not easy to understand. Some data may have data entry errors, and some variables may need data type conversion. We need to fix this issue in the data.</p> <p><code>In the example</code>, The brand name \u2018Isuzu\u2019 \u2018ISUZU\u2019 and \u2018Mini\u2019 and \u2018Land\u2019 looks incorrect. This needs to be corrected</p> <p>As you can see below \ud83d\udc47</p> <p></p> <p>So </p> <pre><code>searchfor = ['Isuzu' ,'ISUZU','Mini','Land']\ndata[data.Brand.str.contains('|'.join(searchfor))].head(5) ## run this cell\n\n# hence we'll replace the word and will correct it\n\ndata[\"Brand\"].replace({\"ISUZU\": \"Isuzu\", \"Mini\": \"Mini Cooper\",\"Land\":\"Land Rover\"}, inplace=True)\n</code></pre>"},{"location":"Day%204/#separate-something","title":"Separate something \ud83c\uddfe \ud83c\uddfe","text":"<p>Before moving further to univariate and bivariate analysis, let's separate numerical and categorical features.</p> <pre><code>cat_cols=data.select_dtypes(include=['object']).columns\nnum_cols = data.select_dtypes(include=np.number).columns.tolist()\n</code></pre>"},{"location":"Day%204/#univariate-analysis","title":"Univariate Analysis \ud83e\ude90","text":"<p>Now Univariate analysis means analyzing every feature of the dataset so that we'll get clarity regarding preprocessing things for each features. We'll create some distribution charts like</p> <ul> <li>Skew value to understand that is our distribution of numerical feature is normal (bell curve) or not (biased left or right).</li> <li>Histogram is a graph plot to define the distribution of each feature (we can also use kernle plot).</li> <li>Box plot to understand that feature contains outliers or not.</li> </ul> <pre><code>for col in num_cols:\n    print(col)\n    print('Skew :', round(data[col].skew(), 2))\n    plt.figure(figsize = (15, 4))\n    plt.subplot(1, 2, 1)\n    data[col].hist(grid=False)\n    plt.ylabel('count')\n    plt.subplot(1, 2, 2)\n    sns.boxplot(x=data[col])\n    plt.show()\n</code></pre> <p>Output</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p><code>Price</code> and <code>Kilometers Driven</code> are right skewed for this data to be transformed, and all outliers will be handled during imputation</p> <p>Categorical variables are being visualized using a count plot. Categorical variables provide the pattern of factors influencing car price.</p> <pre><code>fig, axes = plt.subplots(3, 2, figsize = (18, 18))\nfig.suptitle('Bar plot for all categorical variables in the dataset')\nsns.countplot(ax = axes[0, 0], x = 'Fuel_Type', data = data, color = 'blue', \n              order = data['Fuel_Type'].value_counts().index);\nsns.countplot(ax = axes[0, 1], x = 'Transmission', data = data, color = 'blue', \n              order = data['Transmission'].value_counts().index);\nsns.countplot(ax = axes[1, 0], x = 'Owner_Type', data = data, color = 'blue', \n              order = data['Owner_Type'].value_counts().index);\nsns.countplot(ax = axes[1, 1], x = 'Location', data = data, color = 'blue', \n              order = data['Location'].value_counts().index);\nsns.countplot(ax = axes[2, 0], x = 'Brand', data = data, color = 'blue', \n              order = data['Brand'].head(20).value_counts().index);\nsns.countplot(ax = axes[2, 1], x = 'Model', data = data, color = 'blue', \n              order = data['Model'].head(20).value_counts().index);\naxes[1][1].tick_params(labelrotation=45);\naxes[2][0].tick_params(labelrotation=90);\naxes[2][1].tick_params(labelrotation=90);\n</code></pre> <p>Output</p> <p></p> <p></p> <p></p> <p>From the count plot, we can have below observations</p> <ul> <li>Mumbai has the highest number of cars available for purchase, followed by Hyderabad and Coimbatore</li> <li>53% of cars have fuel type as Diesel this shows diesel cars provide higher performance</li> <li>72% of cars have manual transmission</li> <li>82 % of cars are First owned cars. This shows most of the buyers prefer to purchase first-owner cars</li> <li>20% of cars belong to the brand Maruti followed by 19% of cars belonging to Hyundai</li> <li>WagonR ranks first among all models which are available for purchase</li> </ul>"},{"location":"Day%204/#data-transformation","title":"Data Transformation \u26d1\ufe0f","text":"<p>Before we proceed to Bi-variate Analysis, Univariate analysis demonstrated the data pattern as some variables to be transformed.</p> <p>Price and Kilometer-Driven variables are highly skewed and on a larger scale. Let\u2019s do log transformation.</p> <p>Log transformation can help in normalization, so this variable can maintain standard scale with other variables:</p> <pre><code># Function for log transformation of the column\ndef log_transform(data,col):\n    for colname in col:\n        if (data[colname] == 1.0).all():\n            data[colname + '_log'] = np.log(data[colname]+1)\n        else:\n            data[colname + '_log'] = np.log(data[colname])\n    data.info()\nlog_transform(data,['Kilometers_Driven','Price'])\n\n#Log transformation of the feature 'Kilometers_Driven'\nsns.distplot(data[\"Kilometers_Driven_log\"], axlabel=\"Kilometers_Driven_log\")\n</code></pre> <p>Output</p> <p></p> <p></p>"},{"location":"Day%204/#bivariate-analysis","title":"Bivariate Analysis \u270c\ufe0f","text":"<p>Now, let\u2019s move ahead with bivariate analysis. Bivariate Analysis helps to understand how variables are related to each other and the relationship between dependent and independent variables present in the dataset.</p> <p>For Numerical variables, Pair plots and Scatter plots are widely been used to do Bivariate Analysis.</p> <p>A Stacked bar chart can be used for categorical variables if the output variable is a classifier. Bar plots can be used if the output variable is continuous</p> <p>In our example, a pair plot has been used to show the relationship between two Categorical variables.</p> <p></p> <p>Pair Plot provides below insights:</p> <ul> <li>The variable Year has a positive correlation with price and mileage</li> <li>A year has a Negative correlation with kilometers-Driven</li> <li>Mileage is negatively correlated with Power</li> <li>As power increases, mileage decreases</li> <li>Car with recent make is higher at prices. As the age of the car increases price decreases</li> <li>Engine and Power increase, and the price of the car increases</li> </ul> <p>A bar plot can be used to show the relationship between Categorical variables and continuous variables.</p> <p><pre><code>fig, axarr = plt.subplots(4, 2, figsize=(12, 18))\ndata.groupby('Location')['Price_log'].mean().sort_values(ascending=False).plot.bar(ax=axarr[0][0], fontsize=12)\naxarr[0][0].set_title(\"Location Vs Price\", fontsize=18)\ndata.groupby('Transmission')['Price_log'].mean().sort_values(ascending=False).plot.bar(ax=axarr[0][1], fontsize=12)\naxarr[0][1].set_title(\"Transmission Vs Price\", fontsize=18)\ndata.groupby('Fuel_Type')['Price_log'].mean().sort_values(ascending=False).plot.bar(ax=axarr[1][0], fontsize=12)\naxarr[1][0].set_title(\"Fuel_Type Vs Price\", fontsize=18)\ndata.groupby('Owner_Type')['Price_log'].mean().sort_values(ascending=False).plot.bar(ax=axarr[1][1], fontsize=12)\naxarr[1][1].set_title(\"Owner_Type Vs Price\", fontsize=18)\ndata.groupby('Brand')['Price_log'].mean().sort_values(ascending=False).head(10).plot.bar(ax=axarr[2][0], fontsize=12)\naxarr[2][0].set_title(\"Brand Vs Price\", fontsize=18)\ndata.groupby('Model')['Price_log'].mean().sort_values(ascending=False).head(10).plot.bar(ax=axarr[2][1], fontsize=12)\naxarr[2][1].set_title(\"Model Vs Price\", fontsize=18)\ndata.groupby('Seats')['Price_log'].mean().sort_values(ascending=False).plot.bar(ax=axarr[3][0], fontsize=12)\naxarr[3][0].set_title(\"Seats Vs Price\", fontsize=18)\ndata.groupby('Car_Age')['Price_log'].mean().sort_values(ascending=False).plot.bar(ax=axarr[3][1], fontsize=12)\naxarr[3][1].set_title(\"Car_Age Vs Price\", fontsize=18)\nplt.subplots_adjust(hspace=1.0)\nplt.subplots_adjust(wspace=.5)\nsns.despine()\n</code></pre> Output</p> <p></p> <p></p> <p></p> <p></p> <ul> <li> <p>Observations</p> <ul> <li>The price of cars is high in Coimbatore and less price in Kolkata and Jaipur</li> <li>Automatic cars have more price than manual cars.</li> <li>Diesel and Electric cars have almost the same price, which is maximum, and LPG cars have the lowest price</li> <li>First-owner cars are higher in price, followed by a second</li> <li>The third owner\u2019s price is lesser than the Fourth and above</li> <li>Lamborghini brand is the highest in price</li> <li>Gallardocoupe Model is the highest in price</li> <li>2 Seater has the highest price followed by 7 Seater</li> <li>The latest model cars are high in price</li> </ul> </li> </ul>"},{"location":"Day%204/#multivariate-analysis","title":"Multivariate Analysis \ud83d\ude84","text":"<p>As the name suggests, Multivariate analysis looks at more than two variables. Multivariate analysis is one of the most useful methods to determine relationships and analyze patterns for any dataset.</p> <p>A heat map is widely been used for Multivariate Analysis</p> <p>Heat Map gives the correlation between the variables, whether it has a positive or negative correlation.</p> <p>In our example heat map shows the correlation between the variables.</p> <pre><code>plt.figure(figsize=(12, 7))\nsns.heatmap(data.drop(['Name','Location','Transmission','Owner_Type','Mileage','Engine','Power','Brand','New_Price','Model','Fuel_Type','Kilometers_Driven','Price'],axis=1).corr(), annot = True, vmin = -1, vmax = 1)\nplt.show()\n</code></pre> <p>Output</p> <p></p>"},{"location":"Day%204/#finished","title":"Finished  \ud83c\udf8a\ud83c\udf8a","text":"<p>We've reached to end of EDA. To be honest, there are lots of things and different exploration regarding different dataset and usecases. As we learn gradually, we'll learn differen techniques</p>"},{"location":"Day%205/","title":"Day 5","text":""},{"location":"Day%205/#different-techniques-to-impute","title":"Different Techniques to impute \u26c8\ufe0f \u26c8\ufe0f","text":"<p>As we know, missing data in a dataset is a big NO \u274c\u274c Because of the following reasons - </p> <ul> <li>It leads to poor performance of any Machine Learning models \ud83d\udc4e</li> <li>Distribution of the features become distorted \ud83e\ude85</li> </ul> <p>Dealing with missing data is a common and inherent issue in data collection, especially when working with large datasets. There are various reasons for missing data, such as incomplete information provided by participants, non-response from those who decline to share information, poorly designed surveys, or removal of data for confidentiality reasons.</p> <p>When not appropriately handled, missing data can bias the conclusions of all the statistical analyses on the data, leading the business to make wrong decisions.</p>"},{"location":"Day%205/#types-of-missing-data","title":"Types of missing data \ud83c\udd8e","text":"<p>Missing data occurs in different formats. This section explains the different types of missing data and how to identify them.</p>"},{"location":"Day%205/#mcar-missing-completely-at-random","title":"MCAR - Missing completely at random \ud83c\udf11","text":"<p>This happens if all the variables and observations have the same probability of being missing. Imagine providing a child with Lego of different colors to build a house. Each Lego represents a piece of information, like shape and color. The child might lose some Legos during the game. These lost legos represent missing information, just like when they can\u2019t remember the shape or the color of the Lego they had. That information was lost randomly, but they do not change the information the child has on the other Legos. <code>Inshort , data that we've lost or missing is not harming or related to any other feature.</code></p>"},{"location":"Day%205/#mar-missing-at-random","title":"MAR - Missing at random \ud83c\udf19","text":"<p>For MAR, the probability of the value being missing is related to the value of the variable or other variables in the dataset. This means that not all the observations and variables have the same chance of being missing. An example of MAR is a survey in the Data community where data scientists who do not frequently upgrade their skills are more likely not to be aware of new state-of-the-art algorithms or technologies, hence skipping certain questions. The missing data, in this case, is related to how frequently the data scientist upskills. <code>Like here, if we are not updating our knowledge, we'll go to downfall or become jobless means missing value is related (partially) somewhere.</code></p>"},{"location":"Day%205/#mnar-missing-not-at-random","title":"MNAR- Missing not at random \u2600\ufe0f","text":"<p>MNAR is considered to be the most difficult scenario among the three types of missing data. It is applied when neither MAR nor MCAR apply. In this situation, the probability of being missing is completely different for different values of the same variable, and these reasons can be unknown to us. An example of MNAR is a survey about married couples. Couples with a bad relationship might not want to answer certain questions as they might feel embarrassed to do so. <code>We can't find out, why values are missing or any relation.</code></p>"},{"location":"Day%205/#how-to-identify-missing-value","title":"How to identify missing value \ud83d\udc53","text":"<p>Basically we used these function to find out any missing data in a features :- </p> Functions Descriptions .isnull() This function returns a pandas dataframe, where each value is a boolean value True if the value is missing, False otherwise. .notnull() Similarly to the previous function, the values for this one are False if either NaN or None value is detected. .info() This function generates three main columns, including the \u201cNon-Null Count\u201d which shows the number of non-missing values for each column. .isna() This one is similar to isnull and notnull. However it shows True only when the missing value is NaN type."},{"location":"Day%205/#data","title":"Data \ud83d\udcbe","text":"<p>So new day, new data \ud83d\ude06, We will look at the Telecom customer churn dataset which we'll look into more detail while handling missing data below</p>"},{"location":"Day%205/#handle-missing-data","title":"Handle Missing data \ud83e\ude9b","text":"<p>Since the dataset does not have any missing values,  we will use a subset of the data (100 rows) and then manually introduce missing values.</p>"},{"location":"Day%205/#import-library-and-read-the-data","title":"Import library and read the data \ud83d\udefb","text":"<pre><code>import pandas as pd\nsample_customer_data = pd.read_csv(\"data/customer_churn.csv\",  nrows=100)\nsample_customer_data.info()\n</code></pre> <p>Columns we have in the data and their datatypes</p> <p></p> <p>Let\u2019s introduce 50% of missing values in each column of the dataframe using.</p> <pre><code>import numpy as np\ndef introduce_nan(x,percentage):\nn = int(len(x)*(percentage - x.isna().mean()))\nidxs = np.random.choice(len(x), max(n,0), replace=False, p=x.notna()/x.notna().sum())\nx.iloc[idxs] = np.nan\n</code></pre> <p>Now after applying this function to the data, we've generated missing data in our data intentionally.</p> <pre><code>sample_customer_data.apply(introduce_nan, percentage=.5)\nsample_customer_data.info()\n</code></pre> <p>After adding missing data , Output</p> <p></p>"},{"location":"Day%205/#data-dropping","title":"Data dropping \ud83d\udca7","text":"<p>Using the dropna() function is the easiest way to remove observations or features with missing values from the dataframe. Below are some techniques. </p> <ul> <li>Drop observations with missing values</li> </ul> <p>These three scenarios can happen when trying to remove observations from a data set: </p> Scenarios Code Output dropna() drops all the rows with missing values. <code>drop_na_strategy = sample_customer_data.dropna()</code> We can see that all the observations are dropped from the dataset, which can be especially dangerous for the rest of the analysis.  dropna(how = \u2018all\u2019) the rows where all the column values are missing. `drop_na_all_strategy = sample_customer_data.dropna(how=\"all\") ` dropna(thresh = minimum_value) drop rows based on a threshold. This strategy sets a minimum number of missing values required to preserve the rows. <code>drop_na_thres_strategy = sample_customer_data.dropna(thresh=0.6)</code> Setting the threshold to 60%, the result is the same compared to the previous one. <ul> <li>Drop columns with missing data</li> </ul> <p>The parameter axis = 1 can be used to explicitly specify we are interested in columns rather than rows. </p> <p><pre><code>drop_na_cols_strategy = sample_customer_data.dropna(axis=1)\ndrop_na_cols_strategy.info()\n</code></pre> There are no more columns in the data. This is because all the columns have at least one missing value. Hence Output looks like</p> <p></p> <p>Like many other approaches, <code>dropna()</code> also has some pros and cons.</p> Pros Cons - Straightforward and simple to use.- Beneficial when missing values have no importance. - Using this approach can lead to information loss, which can introduce bias to the final dataset.- This is not appropriate when the data is not missing completely at random.- Data set with a large proportion of missing value can be significantly decreased, which can impact the result of all statistical analysis on that data set."},{"location":"Day%205/#meanmedian-imputation","title":"Mean/Median Imputation \ud83c\udfe6","text":"<p>These replacement strategies  are self-explanatory. Mean and median imputations are respectively used to replace missing values of a given column with the mean and median of the non-missing values in that column. Normal distribution is the ideal scenario. Unfortunately, it is not always the case. This is where the median imputation can be helpful because it is not sensitive to outliers. In Python, the <code>fillna()</code> function from pandas can be used to make these replacements. </p> <ul> <li>Illustration of Mean imputation</li> </ul> <pre><code>mean_value = sample_customer_data.mean()\nmean_imputation = sample_customer_data.fillna(mean_value)\n</code></pre> <ul> <li>Illustration of Median imputation</li> </ul> <pre><code>median_value = sample_customer_data.median()\nmedian_imputation = sample_customer_data.fillna(median_value)\nmedian_imputation.head()\n</code></pre> <p>We can use any strategy and analyze the distribution of feature that which strategy is working best !!</p> <p>Let's see pros and cons of these strategy :-</p> Pros Cons - Simplicity and ease of implementation are some of the benefits of the mean and median imputation.- The imputation is performed using the existing information from the non-missing data; hence no additional data is required.- Mean and median imputation can provide a good estimate of the missing values, respectively for normally distributed data, and skewed data. - We cannot apply these two strategies to categorical columns. They can only work for numerical ones.- Mean imputation is sensitive to outliers and may not be a good representation of the central tendency of the data. Similarly to the mean, the median also may not better represent the central tendency.- Median imputation makes the assumption that the data is missing completely at random (MCAR), which is not always true."},{"location":"Day%205/#random-sample-imputation","title":"Random Sample Imputation \ud83e\uddea","text":"<p>The idea behind the random sample imputation is different from the previous ones and involves additional steps. </p> <ul> <li>First, it starts by creating two subsets from the original data. </li> <li>The first subset contains all the observations without missing data, and the second one contains those with missing data. </li> <li>Then, it randomly selects from each subset a random observation.</li> <li>Furthermore, the missing data from the previously selected observation is replaced with the existing ones from the observation having all the data available.</li> <li>Finally, the process continues until there is no more missing information.</li> </ul> <pre><code>def random_sample_imputation(df):\n\ncols_with_missing_values = df.columns[df.isna().any()].tolist()\n\nfor var in cols_with_missing_values:\n\n    # extract a random sample\n    random_sample_df = df[var].dropna().sample(df[var].isnull().sum(),\n                                                  random_state=0)\n    # re-index the randomly extracted sample\n    random_sample_df.index = df[\n            df[var].isnull()].index\n\n    # replace the NA\n    df.loc[df[var].isnull(), var] = random_sample_df\n\nreturn df\n\ndf = sample_customer_data.copy()\nrandom_sample_imp_df = random_sample_imputation(sample_customer_data)\nrandom_sample_imp_df.head()\n</code></pre> <p>Output</p> <p>Go and implement lazy \ud83d\ude06\ud83d\ude06</p> Pros Cons - This is an easy and straightforward technique.- It tackles both numerical and categorical data types. - There is less distortion in data variance, and it also preserves the original distribution of the data, which is not the case for mean, median, and more. - The randomness does not necessarily work for every situation, and this can infuse noise in the data, hence leading to incorrect statistical conclusions.- Similarly to the mean and median, this approach also assumes that the data is missing completely at random (MCAR)."},{"location":"Day%205/#multivariate-imputation","title":"Multivariate imputation \ud83e\uddea\ud83e\uddea","text":"<p>This is a multivariate imputation technique, meaning that the missing information is filled by taking into consideration the information from the other columns. </p> <p>For instance, if the income value is missing for an individual, it is uncertain whether or not they have a mortgage. So, to determine the correct value, it is necessary to evaluate other characteristics such as credit score, occupation, and whether or not the individual owns a house.</p> <p>Multiple Imputation by Chained Equations (MICE for short) is one of the most popular imputation methods in multivariate imputation. To better understand the MICE approach, let\u2019s consider the set of variables X1, X2, \u2026 Xn, where some or all have missing values. </p> <p>The algorithm works as follows: </p> <ul> <li>For each variable, replace the missing value with a simple imputation strategy such as mean imputation, also considered as \u201cplaceholders.\u201d</li> <li>The \u201cplaceholders\u201d for the first variable, X1, are regressed by using a regression model where X1 is the dependent variable, and the rest of the variables are the independent variables. Then X2 is used as dependent variables and the rest as independent variables. The process continues as such until all the variables are considered at least once as the dependent variable.</li> <li>Those original \u201cplaceholders\u201d are then replaced with the predictions from the regression model.</li> <li>The replacement process is repeated for a number of cycles which is generally ten, according to Raghunathan et al. 2002, and the imputation is updated at each cycle. </li> <li>At the end of the cycle, the missing values are ideally replaced with the prediction values that best reflect the relationships identified in the data.</li> </ul> <p>The implementation is performed using the miceforest library. </p> <p>First, we need to install the library using the pip.</p> <pre><code>pip install miceforest\n</code></pre> <p>Then we import the ImputationKernel module and create the kernel for imputation.</p> <pre><code>from miceforest import ImputationKernel\n\nmice_kernel = ImputationKernel(\ndata = sample_customer_data,\nsave_all_iterations = True,\nrandom_state = 2023\n)\n</code></pre> <p>Furthermore, we run the kernel on the data for two iterations, and finally, create the imputed data.</p> <pre><code>mice_kernel.mice(2)\nmice_imputation = mice_kernel.complete_data()\nmice_imputation.head()\n</code></pre> Pros Cons - Multiple imputation is powerful at dealing with missing data in multiple variables and multiple data types.- The approach can produce much better results than mean and median imputations.- Many other algorithms, such as K-Nearest Neighbors, Random forest, and neural networks, can be used as the backbone of the multiple imputation prediction for making predictions. - Multiple imputation assumes that the data is missing at random (MAR).- Despite all the benefits, this approach can be computationally expensive compared to other techniques, especially when working with large datasets.- This approach requires more effort than the previous ones. <p>From all the imputations, it is possible to identify which one is closer to the distribution of the original data.</p> <p>The mean (in yellow) and median (in red) are far away from the original distribution of the \u201cCharge amount\u201d column data, hence are not considered to be great for imputing the data.</p> <pre><code>mean_imputation[\"Charge Amount Mean Imp\"] = mean_imputation[\"Charge Amount\"]\nmedian_imputation[\"Charge Amount Median Imp\"] = median_imputation[\"Charge Amount\"]\nrandom_sample_imp_df[\"Charge Amount Random Imp\"] = random_sample_imp_df[\"Charge Amount\"]\n</code></pre> <p>With the new columns created for each type of imputation, we can now plot the distribution.</p> <pre><code>import matplotlib.pyplot as plt\nplt.figure(figsize=(12,8))\n\nsample_customer_data[\"Charge Amount\"].plot(kind='kde',color='blue')\nmean_imputation[\"Charge Amount Mean Imp\"].plot(kind='kde',color='yellow')\nmedian_imputation[\"Charge Amount Median Imp\"].plot(kind='kde',color='red')\n</code></pre> <p>Output</p> <p></p> <p>Plotting the multiple imputation and the random imputation below, these distributions are perfectly overlapped with the original data. This means that those imputations are better than the mean and median imputations.</p> <pre><code>random_sample_imp_df[\"Charge Amount Random Imp\"] = random_sample_imp_df[\"Charge Amount\"]\nmice_imputation[\"Charge Amount MICE Imp\"] = mice_imputation[\"Charge Amount\"]\n\nmice_imputation[\"Charge Amount MICE Imp\"].plot(kind='kde',color='black')\nrandom_sample_imp_df[\"Charge Amount Random Imp\"].plot(kind='kde',color='purple')\n\nplt.legend()\n</code></pre> <p>Output</p> <p></p>"},{"location":"Day%206/","title":"Day 6","text":""},{"location":"Day%206/#different-methods-to-encode","title":"Different methods to Encode","text":"<p>Machine learning model eventually tries to find out the pattern, equation from the data while training hence it only understands data in numerical format. It doesn't understand the textual format. Therefore we also need to convert all the text based column into numerical format but It doesn't mean that we'll assign any number to any label in a text column. We should convert text feature into numerical format in a very meaningful fashion. </p> <p>Let's study the different methods to encode / convert the categorical columns -</p>"},{"location":"Day%206/#types-of-categorical-data","title":"Types of Categorical Data","text":"<p>Categorical data can be considered as the finite possible values that are divided into groups. For Example \u2014 different blood groups, Genders, Different cities, and states. There are two types of categorical data:</p>"},{"location":"Day%206/#ordinal-data","title":"Ordinal Data","text":"<p>Data that comprises a finite set of discrete values with an order or level of preferences. Example \u2014 [Low, Medium, High], [Positive, Negative], [True, False]</p>"},{"location":"Day%206/#nominal-data","title":"Nominal Data","text":"<p>Data that comprises a finite set of discrete values with no relationship between them. Example \u2014 [\u201cIndia\u201d, \u201cAmerica\u201d, \u201cEngland\u201d], [\u201cLion\u201d, \u201cMonkey\u201d, \u201cZebra\u201d]</p> <p>Note - For Ordinal Data, after encoding the data and training the model, we need to transform it to its original form again, as it is needed to properly predict the value. But for Nominal Data, it is not required as here preference doesn\u2019t matter, we just need the information.</p> <p>Let's begin encoding the categorical variables by first knowing about the data</p>"},{"location":"Day%206/#about-data","title":"About Data","text":"<p>New data, new data \ud83d\ude06. We are taking mushroom dataset which consists cap shape, cap diameter, gill spacing etc... properties of mushroom and target column class of mushroom. Let's load and view the info about it.</p> <pre><code>df = pd.read_csv('https://raw.githubusercontent.com/muttinenisairohith/Datasets/main/data/processed_mushroom_dataset.csv')\ndf.info()\n</code></pre> <p>Output</p> <p>As we can see we have 15 columns out of which 12 are of Datatype \u2014 object And there are no missing values in this data. So we need to Encode these 12 features before we go for modeling.</p> <p></p>"},{"location":"Day%206/#label-encoding","title":"Label Encoding","text":"<p>In Label Encoding, each label will be converted into an integer value. Here the output will be 1 dimensional.</p> <p></p> <p>Now we are using an awesome library named scikit-learn, we are using this library in further days alot in many projects. So install it using <code>pip install scikit-learn==&lt;version-name&gt;</code></p> <pre><code>from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf[\"season\"] = le.fit_transform(df[\"season\"])\n</code></pre> <p>After using label encoder, output of season column is like</p> <p></p>"},{"location":"Day%206/#ordinal-encoding","title":"Ordinal Encoding","text":"<p>Similar to label Encoding but ordinal Encoding is generally used when we are intended for input variables that are organized into rows and columns. (eg: Matix)</p> <p></p> <pre><code>from sklearn.preprocessing import OrdinalEncoder\noe = OrdinalEncoder()\nencoded_df[\"season\"] = oe.fit_transform(df[[\"season\"]])\n</code></pre> <p>After using ordinal encoder, output of season column is like</p> <p></p>"},{"location":"Day%206/#one-hot-encoding","title":"One Hot Encoding","text":"<p>In One-Hot Encoding, each category of any categorical variable gets a new variable. It maps each category with binary numbers (0 or 1). This type of encoding is used when the data is nominal. Newly created binary features can be considered dummy variables. After one hot encoding, the number of dummy variables depends on the number of categories presented in the data.</p> <p></p> <p>For One hot encoding, we will be using the category_encoders package instead of sklearn, as it is more useful.</p> <p>To install category_encoders, use <code>pip install category_encoders</code></p> <p><pre><code>from category_encoders import OneHotEncoder\nohe = OneHotEncoder( handle_unknown='return_nan', return_df=True, use_cat_names=True)\nohe_results = ohe.fit_transform(df[[\"season\"]])\n</code></pre> After using category encoders, output of season column is like</p> <p></p>"},{"location":"Day%206/#conclusion","title":"Conclusion","text":"<p>There are also other Encoding libraries that can be used for encoding and internal functions in pandas such as map, replace, apply can also be used for encoding, But the methods provided above are easy ways to encode the data and inverse transform it in time. So I have skipped those methods.</p>"},{"location":"Day%207/","title":"Day 7","text":""},{"location":"Day%207/#different-methods-to-remove-outliers","title":"Different methods to remove outliers \ud83c\udfee","text":""},{"location":"Day%207/#what-are-outliers","title":"What are Outliers ? \u26e9\ufe0f","text":"<p>An outlier is an extreme value that lies at an abnormal distance from other points in the dataset. Dealing with outliers is tricky because sometimes, it can reveal abnormal cases or individuals with rare traits. It can also distort statistical analyses and violate their assumptions. In general, machine learning modeling and model skills can be improved by understanding and even removing these outlier values. Hence it is sometimes recommended to remove outliers from data before feeding it to the machine learning model. </p> <p>As mentioned earlier, outliers are extreme values present in data. One can look into small data and decide whether it is an outlier. Consider an array=<code>[12,5, 9, 11, 72, 7, 61]</code>. In this data, <code>72</code> and <code>61</code> can be considered an outlier.</p> <p></p> <p>Most of the time, data scientists deal with larger-size data. Hence, identifying outliers from data looking at a glance is not possible. Instead, there are different methods to identify and decide whether the data point is an outlier. Sometimes, a sample that lies beyond three standard deviations of the data is considered an outlier. InterQuartile Range (IQR) is also used to identify possible outliers. IQR is used to measure variability by dividing a data set into quartiles. The data is sorted in ascending order and split into three equal parts. Q1, Q2, and Q3, the first, second, and third quartiles, are the values that separate the four equal parts. Q1, Q2, and Q3 represent the data's 25th percentile, 50th percentile, and 75th percentile, respectively. IQR is the range between the first and the third quartiles, namely Q1 and Q3: IQR = Q3 \u2013 Q1. The data points that fall below Q1 \u2013 1.5 IQR or above Q3 + 1.5 IQR are outliers.</p>"},{"location":"Day%207/#when-should-you-remove-outliers","title":"When should you remove outliers ? \ud83c\udfa8","text":"<p>Outliers sometimes affect the performance of a model, But they also uncover rare traits. Hence, whether to keep an outlier or not, that decision should be made judiciously. Look at the figure shown below:</p> <p></p> <p>Let's look at the image above. First, checking if the error is due to data entry is recommended. Suppose we observe the weight of a student is entered as 350kg in a survey. We can delete the entry as it is due to a manual error. Next, if the outlier is not because of manual error, we need to check its impact on the statistical analysis. Look at the table shown below:</p> <p></p> <p>Here, the last entry seems like an outlier. So here, we perform two regression plots, one without the outlier and one without the outlier.</p> <p>Undoubtedly, the outlier changes the slope of the regression line significantly. Hence, we can safely discard it.</p> <p></p> <p>If the analysis shows that the outlier doesn't impact the result, one needs to check if it contradicts the assumptions made in the analysis. If yes, then again, we can discard the outlier, or there is no harm in keeping the outlier and feeding the data to the model.</p>"},{"location":"Day%207/#methods-to-remove-outliers-code-example","title":"Methods to Remove outliers (code example) \ud83d\udcb3","text":"<p>Several methods can help us to detect the outlier. Once detected, we can quickly eliminate them from our data before feeding the data into a model. Here, we will discuss three different methods to identify and remove outliers.</p>"},{"location":"Day%207/#standard-deviation-method","title":"Standard Deviation Method \ud83c\udf0c","text":"<p>If data follows gaussian or nearby gaussian distribution, the standard deviation can be used as a cut-off for identifying outliers. For data following gaussian distribution, its mean summarises the percentage of values in the sample. For example, 1 Standard Deviation from the mean covers 68% of data, 2 Standard Deviations from the mean cover 95% of data, and 3 Standard Deviations from the mean cover 99.7%.</p> <p></p> <p>Generally, samples that are three standard deviations from the mean can be considered outliers. Occasionally, the data is standardized first using a Z-score. Next, outlier detection can be completed using Z-score cut-off values. A python code is shown below:</p> <pre><code># identify outliers with standard deviation\nfrom numpy.random import seed\nfrom numpy.random import randn\nfrom numpy import mean\nfrom numpy import std\n# seed the random number generator\nseed(1)\n# generate univariate observations\ndata = 5 * randn(10000) + 50\n# calculate summary statistics\ndata_mean, data_std = mean(data), std(data)\n# identify outliers\ncut_off = data_std * 3\nlower, upper = data_mean - cut_off, data_mean + cut_off\n# identify outliers\noutliers = [x for x in data if x &lt; lower or x &gt; upper]\nprint('Identified outliers: %d' % len(outliers))\n# remove outliers\noutliers_removed = [x for x in data if x &gt;= lower and x &lt;= upper]\nprint('Non-outlier observations: %d' % len(outliers_removed))\n</code></pre> <p>Output</p> <pre><code>Identified outliers: 29\nNon-outlier observations: 9971\n</code></pre>"},{"location":"Day%207/#interquartile-range","title":"Interquartile Range \ud83d\udc2c","text":"<p>Training data may only sometimes follow Gaussian distribution or nearly Gaussian distribution. In such cases, the Interquartile Range, or IQR for short, helps to eliminate outliers. The IQR is the difference between the data's 75th and 25th percentiles. It represents the box in a box and whisker plot. IQR-based outlier detection techniques assume that the non-anomalous data points lie at high-density regions, while outliers would occur sparsely.</p> <p></p> <pre><code># identify outliers with interquartile range\nfrom numpy.random import seed\nfrom numpy.random import randn\nfrom numpy import percentile\n# seed the random number generator\nseed(1)\n# generate univariate observations\ndata = 5 * randn(10000) + 50\n# calculate interquartile range\nq25, q75 = percentile(data, 25), percentile(data, 75)\niqr = q75 - q25\nprint('Percentiles: 25th=%.3f, 75th=%.3f, IQR=%.3f' % (q25, q75, iqr))\n# calculate the outlier cutoff\ncut_off = iqr * 1.5\nlower, upper = q25 - cut_off, q75 + cut_off\n# identify outliers\noutliers = [x for x in data if x &lt; lower or x &gt; upper]\nprint('Identified outliers: %d' % len(outliers))\n# remove outliers\noutliers_removed = [x for x in data if x &gt;= lower and x &lt;= upper]\nprint('Non-outlier observations: %d' % len(outliers_removed))\n</code></pre> <p>Output</p> <pre><code>Percentiles: 25th=46.685, 75th=53.359, IQR=6.674\nIdentified outliers: 81\nNon-outlier observations: 9919\n</code></pre>"},{"location":"Day%207/#automatic-outlier-detection-one-class-classification","title":"Automatic Outlier Detection : One class classification \ud83e\udd88","text":"<p>One-class classification is another approach for identifying outliers. A one-class classifier is trained on data without an outlier. Once training is done, it can identify data comprised of both outliers. The local outlier factor(LOF) algorithm works based on the nearest neighbors technique for outlier detection. Here, samples are given a score based on how isolated they are based on their local neighborhood. Examples with the most significant score are more likely to be outliers.</p> <pre><code># evaluate the model on the training dataset with outliers removed\nfrom pandas import read_csv\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.metrics import mean_absolute_error\n# load the dataset\nurl = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\ndf = read_csv(url, header=None)\n# retrieve the array\ndata = df.values\n# split into input and output elements\nX, y = data[:, :-1], data[:, -1]\n# split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n# summarize the shape of the training dataset\nprint(X_train.shape, y_train.shape)\n# identify outliers in the training dataset\nlof = LocalOutlierFactor()\nyhat = lof.fit_predict(X_train)\n# select all rows that are not outliers\nmask = yhat != -1\nX_train, y_train = X_train[mask, :], y_train[mask]\n# summarize the shape of the updated training dataset\nprint(X_train.shape, y_train.shape)\n# fit the model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n# evaluate the model\nyhat = model.predict(X_test)\n# evaluate predictions\nmae = mean_absolute_error(y_test, yhat)\nprint('MAE: %.3f' % mae)\n</code></pre> <p>Output</p> <pre><code>(339, 13) (339,)\n(305, 13) (305,)\nMAE: 3.356\n</code></pre>"},{"location":"Day%208/","title":"Day 8","text":""},{"location":"Day%208/#different-methods-for-feature-selection","title":"Different methods for Feature Selection \ud83d\udcf1","text":"<p>We all may have faced the problem of identifying the important features from a set of given data and removing the irrelevant or less important features which do not contribute much to our decision making in order to achieve better accuracy for our model.</p> <p>In machine learning and statistics, feature selection, also known as variable selection, attribute selection or variable subset selection, is the process of reducing the number of input variables when developing a predictive model. Feature selection techniques are used for several reasons:</p> <ul> <li>It reduces model complexity by dropping some irrelevant features.</li> <li>Helps ML algorithm to train a model faster.</li> <li>Reduction of dimensionality helps in avoid overfitting.</li> </ul>"},{"location":"Day%208/#techniques","title":"Techniques \ud83e\udd4b\ud83e\udd4b","text":"<p>In this , we'll discuss 3 essential techniques :</p> <ul> <li>Univariate Selection</li> <li>Feature Importance</li> <li>Correlation matrix with heatmap</li> </ul> <p>Before Discussing above three techniques let us go through the basic methodologies used for feature selection.</p> <ul> <li>Filter Method:</li> </ul> <p>Filter feature selection methods use statistical techniques to evaluate the relationship between each input variable and the target variable, and these scores are used as the basis to choose (filter) those input variables that will be used in the model.</p> <p>The statistical measures used in filter-based feature selection are generally calculated one input variable at a time with the target variable. As such, they are referred to as univariate statistical measures. This may mean that any interaction between input variables is not considered in the filtering process.</p> <p>Note:- In this case, the existence of correlated predictors makes it possible to select important, but redundant, predictors. The obvious consequences of this issue are that too many predictors are chosen and, as a result, collinearity problems arise. </p> <p></p> <ul> <li>Wrapper Method:</li> </ul> <p>Wrapper feature selection methods create many models with different subsets of input features and select those features that result in the best performing model according to a performance metric. These methods are unconcerned with the variable types, although they can be computationally expensive. Recursive Feature Elimination (RFE) is a good example of a wrapper feature selection method.</p> <p></p> <p>Forward Selection: Forward selection is an iterative method in which we start with having no feature in the model. In each iteration, we keep adding the feature which best improves our model till an addition of a new variable does not improve the performance of the model.</p> <p>Backward Elimination: In backward elimination, we start with all the features and removes the least significant feature at each iteration which improves the performance of the model. We repeat this until no improvement is observed on removal of features.</p> <p>Recursive Feature elimination: It is a greedy optimization algorithm which aims to find the best performing feature subset. It repeatedly creates models and keeps aside the best or the worst performing feature at each iteration. It constructs the next model with the left features until all the features are exhausted. It then ranks the features based on the order of their elimination.</p>"},{"location":"Day%208/#begin-with-data","title":"Begin with Data \ud83c\udfc1","text":"<p>So, now we are looking the mobile price data which helps the machine learning model to classify the range of price of mobile. You can view the data here.</p>"},{"location":"Day%208/#univariate-method","title":"Univariate Method \ud83c\udfc7","text":"<ul> <li> <p>Statistical tests can be used to select those features that have the strongest relationship with the output variable.</p> </li> <li> <p>The scikit-learn library provides the SelectKBest class that can be used with a suite of different statistical tests to select a specific number of features.</p> </li> <li> <p>The example below uses the chi-squared (chi\u00b2) statistical test for non-negative features to select k (k=10) of the best features from the Mobile Price Range Prediction Dataset.</p> </li> </ul>"},{"location":"Day%208/#view-the-feature-and-target","title":"View the feature and target \ud83d\udc41\ufe0f\u200d\ud83d\udde8\ufe0f","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nmobile_data = pd.read_csv(\"../input/mobile-price-classification/train.csv\")\n\nX = mobile_data.iloc[:,0:20]  #independent variables\ny = mobile_data.iloc[:,-1]    #target variable i.e price range\nmobile_data.head()\n</code></pre> <p>Output</p> <p></p>"},{"location":"Day%208/#fit-the-chi2-method","title":"Fit the chi^2 method \ud83d\udfea","text":"<pre><code>#apply SelectKBest class to extract top 10 best features\n\nBestFeatures = SelectKBest(score_func=chi2, k=10)\nfit = BestFeatures.fit(X,y)\n</code></pre>"},{"location":"Day%208/#view-the-scores-for-each-feature","title":"View the scores for each feature \ud83d\udcaf","text":"<pre><code>df_scores = pd.DataFrame(fit.scores_) ## scores on each feature\ndf_columns = pd.DataFrame(X.columns) # columns corresponding to score\n\n# now merge columns and their scores\nf_Scores = pd.concat([df_columns,df_scores],axis=1) \nf_Scores.columns = ['Specs','Score']\nf_Scores\n</code></pre> <p>To view 10 most essential features</p> <p><pre><code>print(f_Scores.nlargest(10,'Score'))\n</code></pre> </p> <p>Note: Using Above score we can conclude that 'ram' is the most important feature among all the features which is also true in practical scenario.</p>"},{"location":"Day%208/#feature-importance","title":"Feature Importance \ud83d\udd0f","text":"<ul> <li> <p>You can get the feature importance of each feature of your dataset by using the feature importance property of the model.</p> </li> <li> <p>Feature importance gives you a score for each feature of your data, the higher the score more important or relevant is the feature towards your output variable.</p> </li> <li> <p>Feature importance is an inbuilt class that comes with Tree Based Classifiers, but here in this example I will be using XGB Classifier for extracting the top 10 features for the dataset.</p> </li> </ul> <pre><code>import xgboost\nimport matplotlib.pyplot as plt\n\nmodel = xgboost.XGBClassifier()\nmodel.fit(X,y)\nprint(model.feature_importances_) \n</code></pre> <p>Output</p> <p></p>"},{"location":"Day%208/#plot-the-graph-of-feature-importances-for-better-visualization","title":"plot the graph of feature importances for better visualization  \ud83d\udcb9","text":"<pre><code>feat_imp = pd.Series(model.feature_importances_, index=X.columns)\nfeat_imp.nlargest(10).plot(kind='barh')\n\nplt.figure(figsize=(8,6))\nplt.show()\n</code></pre> <p>Output</p> <p></p>"},{"location":"Day%208/#correlation-matrix","title":"Correlation Matrix \ud83d\udd6f\ufe0f\ud83d\udd6f\ufe0f","text":"<ul> <li> <p>Correlation states how the features are related to each other or the target variable.</p> </li> <li> <p>Correlation can be positive (increase in one value of feature increases the value of the target variable) or negative (increase in one value of feature decreases the value of the target variable)</p> </li> <li> <p>Heatmap makes it easy to identify which features are most related to the target variable, we will plot heatmap of correlated features using the seaborn library.</p> </li> </ul> <pre><code>import seaborn as sns\n\n#get correlations of each features in dataset\ncorrmat = mobile_data.corr()\ntop_corr_features = corrmat.index\n\nplt.figure(figsize=(20,20))\n\n#plot heat map\ng=sns.heatmap(mobile_data[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n</code></pre> <p>Output</p> <p></p> <p>Note: From above correlation plot we can conclude that the feature 'price_range' and 'ram' are highly correlated features which can also be related with the present situation. As ram of your mobile increases price also gets increases. </p>"},{"location":"Day%209/","title":"Day 9","text":""},{"location":"Day%209/#build-data-preprocessing-pipeline","title":"Build Data Preprocessing Pipeline \u2697\ufe0f \u2697\ufe0f","text":"<p>Today we will going to learn -</p> <ul> <li>Reproduce transformations easily on any dataset.</li> <li>Easily track all transformations you apply to your dataset.</li> <li>Start building your library of transformations which you can use later</li> </ul>"},{"location":"Day%209/#begin-with-data","title":"Begin with Data \ud83c\uddf7\ud83c\uddea","text":"<p>For all of these examples, I will be using the airbnb NYC listings dataset from insideairbnb . This is a real dataset containing information scraped from airbnb and has all the information related to a listing on the site.</p> <p>Let us imagine we want to predict the price of a listing given some variables like the property type and neighborhood.</p> <pre><code>raw_data = pd.read_csv('http://data.insideairbnb.com/united-states/ny/new-york-city/2023-09-05/data/listings.csv.gz',compression='gzip')\n\n# View the data\nraw_data.head() # data can't be showed fully, so run by yourself on your notebook \n</code></pre> <p>Let\u2019s start by getting our categorical and numerical variables that we want to work with. We will keep it simple by removing data with missing values in our categorical variables of interest and with no reviews.</p> <pre><code>#Categorical variables to use\ncat_vars = [col for col in raw_data.columns if raw_data[col].dtypes=='O']\n\n# Numerical Variables to use\nnum_vars = [col for col in raw_data.columns if col not in cat_vars]\n\nairbnb_data = raw_data\n\n# Drop observations if categorical data is missing\nbefore = airbnb_data.shape[0]\nairbnb_data = airbnb_data.dropna(subset=cat_vars + [\"number_of_reviews\"])\nairbnb_data = airbnb_data[airbnb_data['number_of_reviews']&gt;0]\nafter = airbnb_data.shape[0] # rows reduced as we are dropping the whole row if categorical data is missing\n\n\nairbnb_data = airbnb_data[cat_vars + num_vars]\n</code></pre>"},{"location":"Day%209/#imputation","title":"Imputation \ud83d\ude9c \ud83d\ude9c","text":"<p>Is a dataset even real if it isn\u2019t missing data? The reality is that we have to deal with missing data all the time. You will have to decide how to deal with missing data for your specific use</p> <ul> <li>You can dropna() rows with missing data. Might drop too much data.</li> <li>Drop the variable that has missing data. What if you really want that variable?</li> <li>Replace NAs with zero, the mean, median, or some other calculation.</li> </ul> <p>Scikit-Learn provides us with a nice simple class to deal with missing values.</p> <p>Let us impute numerical variables such as price or security deposit with the median. For simplicity, we do this for all numerical variables.</p> <pre><code>from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy=\"median\")# Num_vars is the list of numerical variables \nairbnb_num = airbnb_data[num_vars]\nairbnb_num = imputer.fit_transform(airbnb_num)\n</code></pre> <p>The <code>SimpleImputer</code> class will replace all the missing values with the median. The <code>.fit_transform()</code> method will return a nice numpy array object that is ready to be used with any machine learning method. You can choose different metrics and pass it as an argument.</p>"},{"location":"Day%209/#encoding-categorical-variables","title":"Encoding Categorical Variables \ud83d\ude3e","text":"<p>Numerical variables are pretty straightforward. Let us deal with categorical data that usually comes in strings. In this case, we have to deal with variables such as neighborhood, room_type, bed_type. Machine Learning algorithms work better with numbers, so we will convert our categorical variables.</p> <p>How our categorical data looks:</p> <pre><code>airbnb_data[cat_vars].head()\n</code></pre> <p>Output</p> <p></p> <p>We use the OrdinalEncoder to convert our string data to numbers. Each unique value in the variables will be mapped to a number. E.g Apartment =0, Condominium=1, etc.</p> <pre><code>from  sklearn.preprocessing import OrdinalEncoder\nairbnb_cat = airbnb_data[cat_vars]\nordinal_encoder = OrdinalEncoder()\nairbnb_cat_encoded = ordinal_encoder.fit_transform(airbnb_cat)\nairbnb_cat_encoded[:,1:10]\n</code></pre> <p>After encoding we get,</p> <p></p> <p>Are we done? Not yet. ML algos can take things a little to literally and think that categories 1 and 2 are more similar than 1 and 19. This might be good for ratings or things where there is an order but how about neighborhoods? Can this simple encoder differentiate the vibes of SoHo and TriBeCa? Does it know all the normies work in Midtown?</p> <p>We can transform our data to have a singular attribute per category. For Example, we create an attribute equal to 1 when the \u2018property_type\u2019 is \u2018House\u2019 and 0 otherwise. Repeat for all other categories.</p> <p>This process is called one-hot encoding. If you come from a stats or econometrics background these are called dummy variables/attributes.</p> <pre><code>\n</code></pre> <p>We'll get the output like below,</p> <p></p> <p>A wild sparse matrix appears! Instead of a NumPy array the encoder returns a sparse matrix which is very convenient when we have several categorical attributes with hundreds of categories.</p> <p>The matrix is full of 0s except for a single 1 per row. This would use a lot of memory to store but the spare matrix is smart and stores only the location of the nonzero elements.</p>"},{"location":"Day%209/#feature-scaling","title":"Feature Scaling \u2696\ufe0f \u2696\ufe0f","text":"<p>It is very important to scale our data when using machine learning algorithms. There are exceptions, but when the variables are in different scales we usually need to scale them. You can read all about it here.</p> <p>A couple of ways to do it:</p> <ul> <li>Min-max scaling: substract the min value and divide by the range (max-min). Values will range from 0 to 1. The MinMaxScaler does this.</li> <li>Standardization: substract mean and divide by standard deviation. You end up with 0 mean and unit variance. Values are not bounded in this case. Standardization is less affected by outliers. We can use StandardScaler.</li> </ul> <pre><code>from sklearn.preprocessing import StandardScaler\nStandardScaler().fit_transform(airbnb_num)\n</code></pre> <p>That was easy!</p>"},{"location":"Day%209/#custom-transformations","title":"Custom Transformations \ud83c\ude02\ufe0f","text":"<p>Scikit-Learn API is very flexible lets you create your own custom \u201ctransformation\u201d that you can easily incorporate into your process. You just need to implement the fit(), transform(), and fit_transform() methods.</p> <p>Adding the <code>TransformerMixin</code> as a base class gets you the fit_transform() method automatically.</p> <p>Here we have a very simple transformer that creates the ratio of rating to number of reviews. You can make these as complicated as you wish.</p> <pre><code>from sklearn.base import BaseEstimator, TransformerMixin\n\nratings_index = -2\nreviews_index = -1\nclass NewVariablesAdder(BaseEstimator, TransformerMixin):\n\n    def __init__(self):\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n    # Make a new variable that is rating divided by number of reviews\n        ratings_over_reviews = X[:,ratings_index]/X[:,reviews_index]\n        return np.c_[X, ratings_over_reviews]\n</code></pre> <p>Checkout the Scikit-Learn documentation for more details and examples.</p>"},{"location":"Day%209/#build-sequential-pipeline","title":"Build sequential pipeline \u2697\ufe0f \u2697\ufe0f","text":"<p>We can finally put everything together! One dataset can require several transformations which vary according to the variable type. These must also be performed in the right order.</p> <p>Scikit-Learn gifts us the Pipeline class to help with this ubiquitous process to write clean and efficient code.</p> <pre><code>from sklearn.pipeline import Pipeline\n\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('add_variables', NewVariablesAdder()),\n    ('std_scaler', StandardScaler())\n])\n\nnum_transformed = num_pipeline.fit_transform(airbnb_num)\n</code></pre> <p>We created a pipeline for our numerical variables. The Pipeline constructor takes in a list of (\u2018Estimator Name\u2019, Estimator()) pairs. All estimators except the last one must have the fit_transform() method. They must be transformers. The names should be informative but you can put whatever you want.</p> <p>Let us complete our pipeline with our categorical data and create our \u201cmaster\u201d Pipeline</p> <pre><code>from sklearn.compose import ColumnTransformer\n\ndata_pipeline = ColumnTransformer([\n    ('numerical', num_pipeline, num_vars),\n    ('categorical', OneHotEncoder(), cat_vars),\n\n])\n\nairbnb_processed = data_pipeline.fit_transform(airbnb_data)\n</code></pre> <p>We can combine different pipelines applied to different sets of variables. Here we are applying our numerical pipeline (Impute, Transform, Scale) to the numerical variables (num_vars is a list of column names) and do hot encoding to our categorical variables (cat_vars is a list of column names).</p> <p>The <code>ColumnTransformer</code> class will do exactly what the name implies. It will apply the pipeline or transformer to a specified list of variables.</p> <p>It takes in a list of tuples with (\u2018name\u2019, transformer/pipeline, \u2018variables list\u2019)</p> <p>Once we have set up and defined our pipeline process, we can apply it to any dataset or fresh new data easily with the <code>fit_transform()</code> method.</p>"},{"location":"Day%209/#conclusion","title":"Conclusion \ud83c\udfc1","text":"<p>Now you know the basics of implementing clean and efficient pipelines that can be easily organized and utilized on different datasets. For more detailed information reference the Scikit-Learn documentation</p> <p>Why stop at pre-processing? In the pipeline you can finish with the training of your model. Here is a quick complete example:</p> <p><pre><code>from sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\n\nX, y = make_classification(random_state=0)\n X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\npipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])\n\n# The pipeline can be used as any other estimator\n# and avoids leaking the test set into the train set\npipe.fit(X_train, y_train)\nPipeline(steps=[('scaler', StandardScaler()), ('svc', SVC())])\npipe.score(X_test, y_test)\n#0.88\n</code></pre> Once you have your data transformation setup, you can include the training as another \u201cestimator\u201d in your pipeline. Your pipeline can be used as any other estimator. SVC() is included at the end and will use the scaled data passed to it. You can then reuse this pipeline for many other machine learning algorithms or datasets.</p>"}]}